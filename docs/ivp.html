<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Numerical Analysis - 10&nbsp; Initial Value Problems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bvp.html" rel="next">
<link href="./chapter09.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Numerical Analysis - 10&nbsp; Initial Value Problems">
<meta property="og:description" content="">
<meta property="og:site_name" content="Numerical Analysis">
<meta name="twitter:title" content="Numerical Analysis - 10&nbsp; Initial Value Problems">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ivp.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Initial Value Problems</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Numerical Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://maths.york.ac.uk/moodle/course/view.php?id=2497" title="Moodle page" class="quarto-navigation-tool px-1" aria-label="Moodle page"><i class="bi bi-mortarboard-fill"></i></a>
    <a href="https://forms.gle/w17c19vWnM7wpLpz7" title="Submit a correction" class="quarto-navigation-tool px-1" aria-label="Submit a correction"><i class="bi bi-bug-fill"></i></a>
    <a href="https://github.com/gustavdelius/NumericalAnalysis/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Numerical-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Errors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Solving nonlinear equations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Solving systems of nonlinear equations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Iterative techniques for solving systems of linear equations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Approximation and interpolation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Numerical integration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Numerical differentiation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">A direct method for solving tridiagonal linear systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ivp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Initial Value Problems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bvp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Boundary Value Problems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pde.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Partial Differential Equations</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ivpintro" id="toc-sec-ivpintro" class="nav-link active" data-scroll-target="#sec-ivpintro"><span class="header-section-number">10.1</span> Introduction</a></li>
  <li><a href="#sec-euler" id="toc-sec-euler" class="nav-link" data-scroll-target="#sec-euler"><span class="header-section-number">10.2</span> Euler’s Method</a>
  <ul class="collapse">
  <li><a href="#the-method" id="toc-the-method" class="nav-link" data-scroll-target="#the-method"><span class="header-section-number">10.2.1</span> The method</a></li>
  <li><a href="#sec-eulererror" id="toc-sec-eulererror" class="nav-link" data-scroll-target="#sec-eulererror"><span class="header-section-number">10.2.2</span> Error bounds</a></li>
  <li><a href="#local-truncation-error." id="toc-local-truncation-error." class="nav-link" data-scroll-target="#local-truncation-error."><span class="header-section-number">10.2.3</span> Local truncation error.</a></li>
  <li><a href="#global-error" id="toc-global-error" class="nav-link" data-scroll-target="#global-error"><span class="header-section-number">10.2.4</span> Global error</a></li>
  <li><a href="#systems-of-odes" id="toc-systems-of-odes" class="nav-link" data-scroll-target="#systems-of-odes"><span class="header-section-number">10.2.5</span> Systems of ODEs</a></li>
  <li><a href="#second-order-odes" id="toc-second-order-odes" class="nav-link" data-scroll-target="#second-order-odes"><span class="header-section-number">10.2.6</span> Second-order ODEs</a></li>
  </ul></li>
  <li><a href="#sec-ivpfundam" id="toc-sec-ivpfundam" class="nav-link" data-scroll-target="#sec-ivpfundam"><span class="header-section-number">10.3</span> Fundamentals</a>
  <ul class="collapse">
  <li><a href="#vectors-and-matrices" id="toc-vectors-and-matrices" class="nav-link" data-scroll-target="#vectors-and-matrices"><span class="header-section-number">10.3.1</span> Vectors and matrices</a></li>
  <li><a href="#calculus-in-several-variables" id="toc-calculus-in-several-variables" class="nav-link" data-scroll-target="#calculus-in-several-variables"><span class="header-section-number">10.3.2</span> Calculus in several variables</a></li>
  <li><a href="#odes" id="toc-odes" class="nav-link" data-scroll-target="#odes"><span class="header-section-number">10.3.3</span> ODEs</a></li>
  </ul></li>
  <li><a href="#sec-onestep" id="toc-sec-onestep" class="nav-link" data-scroll-target="#sec-onestep"><span class="header-section-number">10.4</span> One-Step Difference Methods</a></li>
  <li><a href="#taylor-methods" id="toc-taylor-methods" class="nav-link" data-scroll-target="#taylor-methods"><span class="header-section-number">10.5</span> Taylor Methods</a>
  <ul class="collapse">
  <li><a href="#advantages-and-disadvantages" id="toc-advantages-and-disadvantages" class="nav-link" data-scroll-target="#advantages-and-disadvantages"><span class="header-section-number">10.5.1</span> Advantages and disadvantages</a></li>
  </ul></li>
  <li><a href="#sec-rk" id="toc-sec-rk" class="nav-link" data-scroll-target="#sec-rk"><span class="header-section-number">10.6</span> Runge-Kutta Methods</a>
  <ul class="collapse">
  <li><a href="#motivation-the-modified-euler-method" id="toc-motivation-the-modified-euler-method" class="nav-link" data-scroll-target="#motivation-the-modified-euler-method"><span class="header-section-number">10.6.1</span> Motivation: The Modified Euler Method</a></li>
  <li><a href="#general-runge-kutta-methods" id="toc-general-runge-kutta-methods" class="nav-link" data-scroll-target="#general-runge-kutta-methods"><span class="header-section-number">10.6.2</span> General Runge-Kutta methods</a></li>
  <li><a href="#local-truncation-error-for-second-order-methods" id="toc-local-truncation-error-for-second-order-methods" class="nav-link" data-scroll-target="#local-truncation-error-for-second-order-methods"><span class="header-section-number">10.6.3</span> Local truncation error for second-order methods</a></li>
  <li><a href="#advantages-and-disadvantages-1" id="toc-advantages-and-disadvantages-1" class="nav-link" data-scroll-target="#advantages-and-disadvantages-1"><span class="header-section-number">10.6.4</span> Advantages and disadvantages</a></li>
  </ul></li>
  <li><a href="#sec-rkf" id="toc-sec-rkf" class="nav-link" data-scroll-target="#sec-rkf"><span class="header-section-number">10.7</span> Error Control, Runge-Kutta-Fehlberg Method</a>
  <ul class="collapse">
  <li><a href="#error-control" id="toc-error-control" class="nav-link" data-scroll-target="#error-control"><span class="header-section-number">10.7.1</span> Error control</a></li>
  <li><a href="#the-runge-kutta-fehlberg-method" id="toc-the-runge-kutta-fehlberg-method" class="nav-link" data-scroll-target="#the-runge-kutta-fehlberg-method"><span class="header-section-number">10.7.2</span> The Runge-Kutta-Fehlberg method</a></li>
  </ul></li>
  <li><a href="#multi-step-methods" id="toc-multi-step-methods" class="nav-link" data-scroll-target="#multi-step-methods"><span class="header-section-number">10.8</span> Multi-Step Methods</a>
  <ul class="collapse">
  <li><a href="#predictor-corrector-methods" id="toc-predictor-corrector-methods" class="nav-link" data-scroll-target="#predictor-corrector-methods"><span class="header-section-number">10.8.1</span> Predictor-corrector methods</a></li>
  <li><a href="#advantages-and-disadvantages-2" id="toc-advantages-and-disadvantages-2" class="nav-link" data-scroll-target="#advantages-and-disadvantages-2"><span class="header-section-number">10.8.2</span> Advantages and disadvantages</a></li>
  </ul></li>
  <li><a href="#sec-stiff" id="toc-sec-stiff" class="nav-link" data-scroll-target="#sec-stiff"><span class="header-section-number">10.9</span> Stiff equations</a>
  <ul class="collapse">
  <li><a href="#the-test-equation" id="toc-the-test-equation" class="nav-link" data-scroll-target="#the-test-equation"><span class="header-section-number">10.9.1</span> The test equation</a></li>
  <li><a href="#implementing-the-implicit-trapezoidal-method" id="toc-implementing-the-implicit-trapezoidal-method" class="nav-link" data-scroll-target="#implementing-the-implicit-trapezoidal-method"><span class="header-section-number">10.9.2</span> Implementing the Implicit Trapezoidal Method</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/gustavdelius/NumericalAnalysis/edit/master/ivp.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ivp" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Initial Value Problems</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-ivpintro" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="sec-ivpintro"><span class="header-section-number">10.1</span> Introduction</h2>
<p>The topic of this and the next chapter is to find <em>approximate solutions</em> to <em>ordinary differential equations</em>.</p>
<p>Let us briefly recall what an ordinary differential equation (ODE) is. A rather arbitrarily chosen example for an ODE (here, of second order) is <span id="eq-odeexample"><span class="math display">\[
y''(x) +  4 y'(x) + \sqrt[3]{y(x)} + \cos(x) = 0.
\tag{10.1}\]</span></span> Equations like this are normally satisfied by many functions <span class="math inline">\(y(x)\)</span>: the problem has many solutions. In order to specify a uniquely solvable problem, one needs to fix <em>initial values</em>, i.e., the value of <span class="math inline">\(y\)</span> and its first derivative at some point, say, at <span class="math inline">\(x=0\)</span>: <span id="eq-ivpexample"><span class="math display">\[
y''(x) +  4 y'(x) + \sqrt[3]{y(x)} + \cos(x) = 0, \quad y(0)=1,\; y'(0)=-2.
\tag{10.2}\]</span></span> This is a so-called <em>initial-value problem</em> (IVP). Another variant is to specify the value of <span class="math inline">\(y(x)\)</span>, but not of its derivative, at two different points: <span id="eq-bvpexample"><span class="math display">\[
y''(x) +  4 y'(x) + \sqrt[3]{y(x)} + \cos(x) = 0, \quad y(0)=2,\; y(1)=1.
\tag{10.3}\]</span></span> This is called a <em>boundary value problem</em> (BVP).</p>
<p>Both IVPs and BVPs have a unique solution (under certain mathematical conditions). However, while one can show on abstract grounds that these solutions exist, it is often not practicable to find an explicit expression for them. The best one can hope for is to approximate the solution numerically. This is exactly our topic: to find approximation algorithms for the solutions of IVPs (this chapter) and BVPs (in Chapter -<a href="bvp.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
</section>
<section id="sec-euler" class="level2 page-columns page-full" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-euler"><span class="header-section-number">10.2</span> Euler’s Method</h2>
<p>For studying initial value problems (IVPs) for ordinary differential equations, let us start with the simplest known approximation scheme: <em>Euler’s method</em>. We will focus on direct computations here, and defer more in-depth discussions of the mathematical foundations to later.</p>
<p>We consider an IVP for a first-order equation, of the form <span id="eq-ivpeuler"><span class="math display">\[
y'(x) = f(x,y(x)),  \quad a \leq x \leq b, \quad y(a) = \alpha.
\tag{10.4}\]</span></span> Here <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(\alpha\)</span> are some real numbers, and <span class="math inline">\(f\)</span> is a function <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}\)</span>. (For a concrete example, see <a href="#exm-eulerivp" class="quarto-xref">Example&nbsp;<span>10.1</span></a> below.) Under certain conditions on <span class="math inline">\(f\)</span> — to be discussed in <a href="#sec-ivpfundam" class="quarto-xref"><span>Section 10.3</span></a> — one knows that the problem has a unique solution; that is, that there is exactly one function <span class="math inline">\(y(x)\)</span> defined on <span class="math inline">\([a,b]\)</span> which satisfies <a href="#eq-ivpeuler" class="quarto-xref">Eq.&nbsp;<span>10.4</span></a>. We often call <span class="math inline">\(y(x)\)</span> the <em>exact solution</em> of the IVP. However, it is often difficult or even impossible to find an explicit expression for <span class="math inline">\(y(x)\)</span>. Our aim here is to approximate the solution numerically.</p>
<section id="the-method" class="level3 page-columns page-full" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="the-method"><span class="header-section-number">10.2.1</span> The method</h3>
<div id="fig-euler" class="quarto-figure quarto-figure-left quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-euler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/euler.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-euler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Euler’s method
</figcaption>
</figure>
</div>
<p>The idea behind Euler’s method is as follows (see <a href="#fig-euler" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> for a visualization): We pick <span class="math inline">\(N \in \mathbb{N}\)</span>, and divide the interval <span class="math inline">\([a,b]\)</span> into <span class="math inline">\(N\)</span> subintervals of equal length, <span class="math inline">\(h = (b-a)/N\)</span>. The end points of these intervals are <span id="eq-eulermesh"><span class="math display">\[
x_i = a + i h, \quad i = 0,\ldots,N.
\tag{10.5}\]</span></span></p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside">Terminology: <span class="math inline">\(N\)</span> is called the <strong>number of steps</strong> and <span class="math inline">\(h\)</span> is called the <strong>step size</strong>. The points <span class="math inline">\(x_0,\ldots,x_N\in[a,b]\)</span> are referred to as <strong>mesh points</strong>.</span></div></div>
<p>We will approximate the exact solution <span class="math inline">\(y\)</span> only at the mesh points <span class="math inline">\(x_i\)</span>; that is, we are looking for numbers <span class="math inline">\(w_0,\ldots,w_N\)</span> such that <span class="math inline">\(y(x_i) \approx w_i\)</span>.</p>
<p>The first approximation <span class="math inline">\(w_0\)</span> is easy to choose: We know that <span class="math inline">\(y\)</span> satisfies the initial condition, <span class="math inline">\(y(x_0) = y(a) = \alpha\)</span>. Thus we set <span class="math inline">\(w_0:=\alpha\)</span>; this is even exact.</p>
<p>For finding <span class="math inline">\(w_1 \approx y(x_1)\)</span>, we use linear approximation: <span id="eq-linapprox"><span class="math display">\[
\begin{split}
   y(x_1) &amp;= y(x_0+h) \approx y(x_0) + h y'(x_0) \\
   &amp;\overset{(*)}{=} y(x_0) + h f(x_0,y(x_0)) = w_0 + hf(x_0,w_0).
\end{split}
\tag{10.6}\]</span></span> For the equality <span class="math inline">\((*)\)</span>, we have used that <span class="math inline">\(y\)</span> satisfies the ODE <a href="#eq-ivpeuler" class="quarto-xref">Eq.&nbsp;<span>10.4</span></a>.</p>
<p>In the same way, we can find an approximation for <span class="math inline">\(y(x_2)\)</span>: <span id="eq-linapprox2"><span class="math display">\[
\begin{split}
   y(x_2) &amp;= y(x_1+h) \approx y(x_1) + h y'(x_1) \\
   &amp;\overset{(*)}{=} y(x_1) + h f(x_1,y(x_1)) \overset{(\circ)}{\approx} w_1 + hf(x_1,w_1).
\end{split}
\tag{10.7}\]</span></span> Our approximation value is <span class="math inline">\(w_2 := w_1 + hf(x_1,w_1)\)</span>. Note that we have used another approximation step <span class="math inline">\((\circ)\)</span>, using that <span class="math inline">\(y(x_1)\approx w_1\)</span> and that <span class="math inline">\(f\)</span> is sufficiently smooth; we will come back to this point in <a href="#sec-onestep" class="quarto-xref"><span>Section 10.4</span></a>. We can continue the scheme for <span class="math inline">\(w_3\)</span>, <span class="math inline">\(w_4\)</span>, etc.: <span id="eq-eulerdiff0"><span class="math display">\[
w_0 := \alpha,
\tag{10.8}\]</span></span> <span id="eq-eulerdiff1"><span class="math display">\[
w_1 := w_0 + h f(x_0,w_0),
\tag{10.9}\]</span></span> <span id="eq-eulerdiff2"><span class="math display">\[
w_2 := w_1 + h f(x_1,w_1),
\tag{10.10}\]</span></span> <span class="math display">\[
\vdots
\]</span> <span id="eq-eulerdiffeq"><span class="math display">\[
w_{i+1} := w_i + h f(x_i,w_i).
\tag{10.11}\]</span></span> So the <span class="math inline">\(w_i\)</span> are defined recursively. Eq.&nbsp;<a href="#eq-eulerdiffeq" class="quarto-xref">Eq.&nbsp;<span>10.11</span></a> is called the <strong>difference equation</strong> of Euler’s method.</p>
<div id="exm-eulerivp" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 10.1</strong></span> Let us consider the IVP <span id="eq-eulerivpexample"><span class="math display">\[
  y'(x) = y(x)-x^2+1,  \quad 0 \leq x \leq 1, \quad y(0) = \frac{1}{2}.
\tag{10.12}\]</span></span> In this case, the exact solution can be found using an integrating factor. It is</p>
<div class="page-columns page-full"><p><span id="eq-eulerexact"><span class="math display">\[
  y(x) = (x+1)^2-\frac{1}{2} e^x.
\tag{10.13}\]</span></span> This will allow us to compare the approximation with the exact solution. </p><div class="no-row-height column-margin column-container"><span class="margin-aside">It is generally a good idea to test one’s numerical method on an example where one already knows the exact solution</span></div></div>
<p>We choose <span class="math inline">\(N=10\)</span> steps, i.e., <span class="math inline">\(h=1/10\)</span>. Here we shall only compute the first two steps. By <a href="#eq-eulerdiff0" class="quarto-xref">Eq.&nbsp;<span>10.8</span></a>, we certainly have <span id="eq-eulerdiff0b"><span class="math display">\[
x_0 = 0, \quad w_0=\frac{1}{2}.
\tag{10.14}\]</span></span> This allows us to compute the approximation <span class="math inline">\(w_1\)</span> at the mesh point <span class="math inline">\(x_1=1/10\)</span>, namely, by <a href="#eq-eulerdiff1" class="quarto-xref">Eq.&nbsp;<span>10.9</span></a>, <span id="eq-eulerdiff1b"><span class="math display">\[
\begin{split}
w_1&amp;=w_0+h f(x_0,w_0)
= w_0 + h (w_0-x_0^2+1)\\
&amp;= \frac{1}{2} + \frac{1}{10} \left(\frac{1}{2}-0+1\right) = \frac{1}{2} + \frac{3}{20}\\
&amp;= \frac{13}{20}.
\end{split}
\tag{10.15}\]</span></span> Continuing the iteration to <span class="math inline">\(x_2=2/10\)</span>, we have by <a href="#eq-eulerdiff2" class="quarto-xref">Eq.&nbsp;<span>10.10</span></a> that <span id="eq-eulerdiff2b"><span class="math display">\[
\begin{split}
  w_2&amp;=w_1+h f(x_1,w_1)
= w_1 + h (w_1-x_1^2+1)
\\
&amp;= \frac{13}{20} + \frac{1}{10} \left(\frac{13}{20}-\left(\frac{1}{10}\right)^2+1 \right)
= \frac{13}{20} + \frac{1}{10} \cdot \frac{174}{100}\\
&amp;= \frac{407}{500}.
\end{split}
\tag{10.16}\]</span></span></p>
<p>Continuing further, we would obtain the values shown in <a href="#tbl-eulerapp" class="quarto-xref">Table&nbsp;<span>10.1</span></a>. The values of the exact solution <a href="#eq-eulerexact" class="quarto-xref">Eq.&nbsp;<span>10.13</span></a>, evaluated to 9 decimals, are added for comparison. The last column, the error bound, will be discussed shortly.</p>
<div class="center">
<div id="tbl-eulerapp" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-eulerapp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.1: Approximation values and errors for Euler’s Method
</figcaption>
<div aria-describedby="tbl-eulerapp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(x_i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(w_i\)</span></th>
<th style="text-align: left;"><span class="math inline">\(y(x_i)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(|y(x_i)-w_i|\)</span></th>
<th style="text-align: left;">error bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">0.65</td>
<td style="text-align: left;">0.657414541</td>
<td style="text-align: left;">0.007414541</td>
<td style="text-align: left;">0.0078878189</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">0.814</td>
<td style="text-align: left;">0.829298621</td>
<td style="text-align: left;">0.015298621</td>
<td style="text-align: left;">0.0166052069</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: left;">0.3</td>
<td style="text-align: left;">0.9914</td>
<td style="text-align: left;">1.015070596</td>
<td style="text-align: left;">0.023670596</td>
<td style="text-align: left;">0.0262394106</td>
</tr>
<tr class="odd">
<td style="text-align: center;">4</td>
<td style="text-align: left;">0.4</td>
<td style="text-align: left;">1.18154</td>
<td style="text-align: left;">1.214087651</td>
<td style="text-align: left;">0.032547651</td>
<td style="text-align: left;">0.0368868524</td>
</tr>
<tr class="even">
<td style="text-align: center;">5</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">1.383694</td>
<td style="text-align: left;">1.425639364</td>
<td style="text-align: left;">0.041945364</td>
<td style="text-align: left;">0.0486540953</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">1.5970634</td>
<td style="text-align: left;">1.648940600</td>
<td style="text-align: left;">0.051877200</td>
<td style="text-align: left;">0.0616589100</td>
</tr>
<tr class="even">
<td style="text-align: center;">7</td>
<td style="text-align: left;">0.7</td>
<td style="text-align: left;">1.82076974</td>
<td style="text-align: left;">1.883123646</td>
<td style="text-align: left;">0.062353906</td>
<td style="text-align: left;">0.0760314530</td>
</tr>
<tr class="odd">
<td style="text-align: center;">8</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;">2.053846714</td>
<td style="text-align: left;">2.127229536</td>
<td style="text-align: left;">0.073382822</td>
<td style="text-align: left;">0.0919155696</td>
</tr>
<tr class="even">
<td style="text-align: center;">9</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">2.295231385</td>
<td style="text-align: left;">2.380198444</td>
<td style="text-align: left;">0.084967059</td>
<td style="text-align: left;">0.1094702333</td>
</tr>
<tr class="odd">
<td style="text-align: center;">10</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">2.543754524</td>
<td style="text-align: left;">2.640859086</td>
<td style="text-align: left;">0.097104562</td>
<td style="text-align: left;">0.1288711371</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-eulererror" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="sec-eulererror"><span class="header-section-number">10.2.2</span> Error bounds</h3>
<p>Denote the error at step <span class="math inline">\(i\)</span> of Euler’s method by <span id="eq-eulererr"><span class="math display">\[
\varepsilon_i=y(x_i)-w_i.
\tag{10.17}\]</span></span> Clearly <span class="math inline">\(\varepsilon_0=0\)</span>. We can represent <span class="math inline">\(\varepsilon_1\)</span> as follows: assuming the second derivative <span class="math inline">\(y''\)</span> exists on <span class="math inline">\((a,b)\)</span>, we can use Taylor’s theorem to give <span id="eq-eulertaylor"><span class="math display">\[
y(x_1)=y(x_0+h)=y(x_0)+hy'(x_0)+\frac{h^2y''(\xi_1)}{2}
\tag{10.18}\]</span></span> for some <span class="math inline">\(\xi_1\in(x_0,x_1)\)</span>. Now, <span class="math inline">\(y\)</span> is a solution to the IVP so <span class="math inline">\(y(x_0)=y(a)=\alpha=w_0\)</span> and <span class="math inline">\(y'(x_0)=f(x_0,y(x_0))=f(x_0,\alpha)=f(x_0,w_0)\)</span> and, by definition of <span class="math inline">\(w_1\)</span>, <span id="eq-eulerw1"><span class="math display">\[
y(x_0)+hy'(x_0)=w_0+hf(x_0,w_0)=w_1.
\tag{10.19}\]</span></span> Substituting this gives <span id="eq-eulererr1"><span class="math display">\[
y(x_1)=w_1+\frac{h^2y''(\xi_1)}{2}.
\tag{10.20}\]</span></span> So the error after one step can be written as <span id="eq-eulererr2"><span class="math display">\[
\varepsilon_1=y(x_1)-w_1=\frac{h^2y''(\xi_1)}{2}.
\tag{10.21}\]</span></span> Although we do not know what <span class="math inline">\(y''(\xi_1)\)</span> is, this does show how the error depends on the step length: it behaves like a multiple of <span class="math inline">\(h^2\)</span>.</p>
<p>We can now make an optimistic guess: if every step contributes a multiple of <span class="math inline">\(h^2\)</span>, then the total error at the end, after <span class="math inline">\(N\)</span> steps, will be some multiple of <span class="math inline">\(Nh^2=(Nh)h=(b-a)h\)</span>. As <span class="math inline">\(h\to 0\)</span>, this tends to zero (whatever the unknown constants are) so the approximate solution converges to the exact solution. It turns out that this is basically correct, although much more careful reasoning is needed, as we shall see when we look at the error after the second step.</p>
<p>We can try to analyse the second step in the same way as the first step: use Taylor’s Theorem to write <span id="eq-eulertaylor2"><span class="math display">\[
y(x_2)=y(x_1+h)=y(x_1)+hy'(x_1)+\frac{h^2y''(\xi_2)}{2}
\tag{10.22}\]</span></span> and use the fact that <span class="math inline">\(y\)</span> is a solution of the DE to substitute <span class="math inline">\(y'(x_1)=f(x_1,y(x_1))\)</span>: <span id="eq-eulertaylor"><span class="math display">\[
y(x_2)=y(x_1)+hf(x_1,y(x_1))+\frac{h^2y''(\xi_2)}{2}.
\tag{10.23}\]</span></span> At this point, things look different: in the first step, we had <span class="math inline">\(y(x_0)=y(a)=\alpha\)</span>, but here we do not have <span class="math inline">\(y(x_1)=w_1\)</span>: the first step starts at <span class="math inline">\((x_0,w_0)=(a,\alpha)\)</span>, which lies exactly on the solution curve, but the second step starts at <span class="math inline">\((x_1,w_1)\)</span>, which does not lie on the solution curve. However, <span class="math inline">\(w_1\)</span> is an approximation to <span class="math inline">\(y(x_1)\)</span>, and we have an expression for the error, namely <span class="math inline">\(\varepsilon_1\)</span>. Substituting <span class="math inline">\(y(x_1)=w_1+\varepsilon_1\)</span> gives <span id="eq-eulererr2"><span class="math display">\[
y(x_2)=w_1+\varepsilon_1+hf(x_1,w_1+\varepsilon_1)+\frac{h^2y''(\xi_2)}{2}.
\tag{10.24}\]</span></span> Now, <span class="math inline">\(w_1+hf(x_1,w_1+\varepsilon_1)\)</span> is very similar to the formula for <span class="math inline">\(w_2\)</span>: the only difference is that it has <span class="math inline">\(w_1+\varepsilon_1\)</span>, instead of <span class="math inline">\(w_1\)</span>. As we did for for <span class="math inline">\(y(x_1)\)</span>, we can think of this as being an approximation plus an error: <span id="eq-eulerlipstart0"><span class="math display">\[
\underbrace{f(x_1,w_1+\varepsilon_1)}_{\text{exact}}=\underbrace{f(x_1,w_1)}_{\text{approx}}+
\underbrace{[f(x_1,w_1+\varepsilon_1)-f(x_1,w_1)]}_{\text{error}}
\tag{10.25}\]</span></span> leading to <span id="eq-eulererr3"><span class="math display">\[
y(x_2)=\varepsilon_1+\underbrace{w_1+hf(x_1,w_1)}_{=w_2}+h[f(x_1,w_1+\varepsilon_1)-f(x_1,w_1)]\frac{h^2y''(\xi_2)}{2},
\tag{10.26}\]</span></span> which, as intended, forces <span class="math inline">\(w_2\)</span> to appear: <span id="eq-eulererr4"><span class="math display">\[
y(x_2)=\varepsilon_1+w_2+h[f(x_1,w_1+\varepsilon_1)-f(x_1,w_1)]+\frac{h^2y''(\xi_2)}{2}.
\tag{10.27}\]</span></span> Subtracting <span class="math inline">\(w_2\)</span> from both sides gives <span id="eq-eulererr5"><span class="math display">\[
\varepsilon_2=\varepsilon_1+h[f(x_1,w_1+\varepsilon_1)-f(x_1,w_1)]+\frac{h^2y''(\xi_2)}{2}.
\tag{10.28}\]</span></span> This describes the error at step 2 as the sum of three terms which can be thought of as:</p>
<ul>
<li><p><span class="math inline">\(\varepsilon_1\)</span>, the error carried forward from step 1;</p></li>
<li><p><span class="math inline">\(h[f(x_1,w_1+\varepsilon_1)-f(x_1,w_1)]\)</span>, the error caused by starting at <span class="math inline">\((x_1,w_1)\)</span>, which is not on the solution curve;</p></li>
<li><p><span class="math inline">\(h^2y''(\xi_2)/2\)</span>, the error built into Euler’s method by the approximation <span class="math inline">\(y(x+h)\approx y(x)+hy'(x)\)</span>.</p></li>
</ul>
<p>This analysis works for every step: we have <span id="eq-eulererr6"><span class="math display">\[
\varepsilon_{i+1}=\varepsilon_i+h[f(x_i,w_i+\varepsilon_i)-f(x_i,w_i)]+\frac{h^2y''(\xi_{i+1})}{2},
\tag{10.29}\]</span></span> where <span class="math inline">\(\xi_{i+1}\in(x_i,x_{i+1})\)</span>. This is a recurrence relation for <span class="math inline">\(\varepsilon_i\)</span>, and <span class="math inline">\(\varepsilon_0=0\)</span>. Now, we need to ask how large the different contributions can be. For the truncation error inherent in Euler’s method, we simply assume that there is a constant <span class="math inline">\(M\)</span> such that <span class="math inline">\(|y''(x)|\leq M\)</span> for all <span class="math inline">\(x\in[a,b]\)</span>. For the error associated with starting away from solution curve, we use Taylor’s Theorem yet again: assuming <span class="math inline">\(f\)</span> is differentiable in the second variable, we can write <span id="eq-eulerlipstart"><span class="math display">\[
f(x_i,w_i+\varepsilon_i)-f(x_i,w_i)=\varepsilon_i\frac{\partial}{\partial z}f(x_i,z)\bigg|_{z=\eta_i}
\tag{10.30}\]</span></span> for some <span class="math inline">\(\eta_i\in(w_i,w_i+\varepsilon_i)\)</span>. We now make the assumption that there is a constant <span class="math inline">\(L\)</span> such that <span id="eq-ldef"><span class="math display">\[
\left|\frac{\partial}{\partial z}f(x,z)\right|\leq L
\tag{10.31}\]</span></span> for all <span class="math inline">\(x\in[a,b]\)</span> and all <span class="math inline">\(z\)</span>. This leads to <span id="eq-eulerlip"><span class="math display">\[
|h[f(x_i,w_i+\varepsilon_i)-f(x_i,w_i)]|=h|f(x_i,w_i+\varepsilon_i)-f(x_i,w_i)|\leq hL|\varepsilon_i|.
\tag{10.32}\]</span></span> Using the triangle inequality on the formula for <span class="math inline">\(\varepsilon_{i+1}\)</span>, we have <span id="eq-eulererr7"><span class="math display">\[
\begin{split}
|\varepsilon_{i+1}|&amp;\leq|\varepsilon_i|+Lh|\varepsilon_i|+\frac{Mh^2}{2}=(1+Lh)|\varepsilon_i|+\frac{Mh^2}{2}\\
&amp;=(1+Lh)|\varepsilon_i|+h\tau(h),
\end{split}
\tag{10.33}\]</span></span> where <span class="math inline">\(\tau(h)=Mh/2\)</span> (this is just a convenient abbreviation, which makes the final answer look tidy). We can apply this estimate repeatedly, starting off with <span class="math inline">\(\varepsilon_0=0\)</span>, to find an estimate for the error after any number of steps. <span id="eq-eulererr8"><span class="math display">\[
\begin{aligned}
|\varepsilon_0| &amp;= 0 \\
|\varepsilon_1| &amp;\leq h\tau(h) \\
|\varepsilon_2| &amp;\leq (1+Lh)|\varepsilon_1|+h\tau(h) \\
         &amp;\leq [(1+Lh)+1]h\tau(h) \\
|\varepsilon_3| &amp;\leq (1+Lh)|\varepsilon_2|+h\tau(h) \\
         &amp;\leq [(1+Lh)^2+(1+Lh)+1]h\tau(h) \\
         &amp;\vdots \\
|\varepsilon_n| &amp;\leq [(1+Lh)^{n-1}+(1+Lh)^{n-2}+\dots+1]h\tau(h).
\end{aligned}
\tag{10.34}\]</span></span> This final formula can be proved by induction. This is a geometric sum, so we can use the standard formula to give for <span class="math inline">\(1\leq n\leq N\)</span> <span id="eq-eulererr9"><span class="math display">\[
|\varepsilon_n|\leq h\tau(h)\sum_{i=0}^{n-1}(1+Lh)^i=h\tau(h)\frac{(1+Lh)^n-1}{Lh}
\tag{10.35}\]</span></span> and the <span class="math inline">\(h\)</span> terms in the numerator and denominator cancel (this is why the abbreviated the Taylor’s Theorem estimate as <span class="math inline">\(h\tau(h)\)</span>). The dependency on <span class="math inline">\(n\)</span> is awkward here, so we use the fact that <span class="math inline">\(1+x\leq e^x\)</span> for all <span class="math inline">\(x\)</span> to replace <span class="math inline">\(1+Lh\)</span> by <span class="math inline">\(e^{Lh}\)</span>: <span id="eq-eulererr10"><span class="math display">\[
|\varepsilon_n|\leq\frac{\tau(h)}{L}(e^{Lhn}-1)''
\tag{10.36}\]</span></span> Now, <span class="math inline">\(hn\)</span> is the distance from <span class="math inline">\(x_0=a\)</span> to <span class="math inline">\(x_n\)</span>, so <span id="eq-eulererr11"><span class="math display">\[
|\varepsilon_n|\leq\frac{\tau(h)}{L}(e^{L(x_n-a)}-1).
\tag{10.37}\]</span></span> The RHS increases exponentially as <span class="math inline">\(x_n\)</span> increases, with the maximum value at <span class="math inline">\(x_n=b\)</span>, so we can finally conclude that <span id="eq-eulergloberr"><span class="math display">\[
|\varepsilon_N|\leq\frac{\tau(h)}{L}(e^{L(x_n-a)}-1)\leq\frac{\tau(h)}{L}(e^{L(b-a)}-1).
\tag{10.38}\]</span></span></p>
<p>In all cases, we see that the error is bounded above by a multiple of <span class="math inline">\(\tau(h)\)</span>, which is itself a multiple of <span class="math inline">\(h\)</span> (because <span class="math inline">\(\tau(h)=Mh/2\)</span>). The multiplier depends on the width of the interval on which we solve the equation but, crucially, if we fix the interval then the multiplier does not change as we decrease the step size. The error therefore tends to zero as <span class="math inline">\(h\to 0\)</span> (equivalently, as <span class="math inline">\(N\to\infty\)</span>), so the approximate solution converges to the exact solution.</p>
<div id="exm-errorbounds" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.2 (Example for error bounds)</strong></span> Let us reconsider <a href="#exm-eulerivp" class="quarto-xref">Example&nbsp;<span>10.1</span></a>, with <span class="math inline">\(f(x,y)= y-x^2+1\)</span>, and compute the error bounds. We have <span class="math inline">\(\partial f / \partial y = 1\)</span>, so we have <span class="math inline">\(L=1\)</span> – see <a href="#eq-ldef" class="quarto-xref">Eq.&nbsp;<span>10.31</span></a>. For the constant <span class="math inline">\(M\)</span>, we use the fact that we know the exact solution: <span class="math inline">\(y(x)=(x+1)^2-e^x/2\)</span>. This gives us <span class="math inline">\(y''(x)=2-e^x/2\)</span>, which, on the interval <span class="math inline">\([0,1]\)</span>, is bounded by <span class="math inline">\(3/2\)</span> (its value at <span class="math inline">\(0\)</span>). So <span class="math inline">\(M = 3/2\)</span>. Inserting, this gives us <span class="math display">\[
|y(x_i)-w_i| \leq \frac{3}{40} \big( e^{(x_i-a)} - 1 \big).
\]</span> These bounds are included in <a href="#tbl-eulerapp" class="quarto-xref">Table&nbsp;<span>10.1</span></a>. You can see that the actual errors are indeed below the bounds, but not very much, particularly for small <span class="math inline">\(x\)</span>.</p>
</div>
<p>Of course, using the exact solution in the error bounds can be considered “cheating” to some extent, since the exact solution of the ODE is in general unknown. There are methods for obtaining estimates for the constant <span class="math inline">\(M\)</span> even if <span class="math inline">\(y(x)\)</span> is not explicitly known, but we will not discuss them at this point.</p>
</section>
<section id="local-truncation-error." class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="local-truncation-error."><span class="header-section-number">10.2.3</span> Local truncation error.</h3>
<p>We see from <a href="#tbl-eulerapp" class="quarto-xref">Table&nbsp;<span>10.1</span></a> that the <span class="math inline">\(w_i\)</span> approximate the exact solution quite reasonably in our example, though the error grows as <span class="math inline">\(i\)</span> increases. We will now try to obtain theoretical bounds on the approximation error, and thus see how fast the method must converge.</p>
<p>The approximation error builds up with each step of the computation. Let us first, for fixed <span class="math inline">\(i\in\{0,\ldots,N-1\}\)</span>, have a look at the error that we make in step <span class="math inline">\(i+1\)</span> (when we compute <span class="math inline">\(w_{i+1}\)</span> from <span class="math inline">\(w_i\)</span>). For simplification, suppose for a moment that the approximation value <span class="math inline">\(w_i\)</span> was actually <em>exact:</em> <span class="math inline">\(y(x_i)=w_i\)</span>. Under this assumption, we have <span class="math display">\[
y(x_{i+1})-w_{i+1} = y(x_{i+1}) - w_i - h f(x_i,w_i)
  = y(x_{i+1}) - y(x_i) - h f(x_i,y(x_i)).
\]</span> Of course, the assumption <span class="math inline">\(y(x_i)=w_i\)</span> is <em>not</em> exactly true, in general. However, one takes this as a motivation to define the <em>local truncation error</em> of the Euler method: <span id="eq-lteeuler"><span class="math display">\[
\tau_{i+1} := \frac{1}{h} \Big\lvert y(x_{i+1}) - y(x_i) - h f\big(x_i,y(x_i)\big) \Big\rvert.
\tag{10.39}\]</span></span></p>
<p>We will see the use of this expression later. Let us try to find an estimate for it.</p>
<p>To that end, we analyze the “linear approximation” of Eq.&nbsp;<a href="#eq-linapprox" class="quarto-xref">Eq.&nbsp;<span>10.6</span></a> a bit more in detail. We suppose that the exact solution <span class="math inline">\(y(x)\)</span> is twice differentiable. Then, we expand it in a Taylor expansion at the point <span class="math inline">\(x_i\)</span>, <span id="eq-eulertaylor"><span class="math display">\[
   y(x_{i+1}) = y(x_i) + h y'(x_i) + \frac{1}{2} h^2  y''(\xi)
\tag{10.40}\]</span></span> where <span class="math inline">\(\xi \in [x_i,x_{i+1}]\)</span> is some intermediate point. Using <span class="math inline">\(y'(x_i) = f(x_i,y(x_i))\)</span>, and inserting into <a href="#eq-lteeuler" class="quarto-xref">Eq.&nbsp;<span>10.39</span></a>, we have <span class="math display">\[
\tau_{i+1} = \frac{1}{h} \cdot \frac{1}{2} h^2  \big\lvert y''(\xi) \big\rvert.
\]</span> Thus, <span id="eq-lteest"><span class="math display">\[
\tau_{i+1} \leq \frac{h}{2} \sup_{x \in [a,b]}\big\lvert y''(x) \big\rvert.
\tag{10.41}\]</span></span></p>
<p>This is a rather simple estimate for the local truncation error. It is proportional to <span class="math inline">\(h\)</span>; one says that the Euler method is <em>of order O(h)</em>. It may seem a bit hard to apply, since <span class="math inline">\(y''(x)\)</span> is in general unknown. See however below.</p>
</section>
<section id="global-error" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="global-error"><span class="header-section-number">10.2.4</span> Global error</h3>
<p>Let us now see how we can find an estimate for the <em>global error</em>, <span class="math display">\[
\epsilon_i := \lvert y(x_{i})-w_i  \rvert.
\]</span> For this, we try to find a recursion relation, i.e., an estimate of <span class="math inline">\(\epsilon_{i+1}\)</span> in terms of <span class="math inline">\(\epsilon_i\)</span>. The triangle inequality yields, <span class="math display">\[
\begin{aligned}
  \epsilon_{i+1} &amp;= \lvert y(x_{i+1})-w_{i+1} - y(x_{i}) + w_i + y(x_i) - w_i \rvert \\
&amp; \leq \lvert y(x_{i+1})-w_{i+1} - y(x_{i}) + w_i  \rvert
  + \lvert y(x_i) - w_i  \rvert \\
&amp; = \lvert y(x_{i+1})-w_{i+1} - y(x_{i}) + w_i  \rvert
  + \epsilon_i.
\end{aligned}
\]</span> We want to relate this to the local truncation error <a href="#eq-lteeuler" class="quarto-xref">Eq.&nbsp;<span>10.39</span></a>. In fact, another application of the triangle inequality gives <span id="eq-eulerlipstart"><span class="math display">\[
  + \epsilon_i + h \tau_{i+1}.
\tag{10.42}\]</span></span> Using the difference equation <a href="#eq-eulerdiffeq" class="quarto-xref">Eq.&nbsp;<span>10.11</span></a> – which defined the approximation values <span class="math inline">\(w_i\)</span> –, we obtain <span class="math display">\[
\epsilon_{i+1} \leq  \epsilon_i + h \tau_{i+1} + h \lvert f(x_{i},y(x_i))-f(x_i,w_i)  \rvert.
\]</span> For handling the rightmost term, we apply Taylor’s theorem to the function <span class="math inline">\(f\)</span> with respect to the second variable: <span class="math display">\[
f(x_{i},y(x_i))-f(x_i,w_i) = (y(x_i)-w_i) \frac{\partial f}{\partial y}(x_i,\eta_i)
\]</span> with some <span class="math inline">\(\eta_i\in\mathbb{R}\)</span>. This finally gives</p>
<p><span id="eq-epsrecur"><span class="math display">\[
   = (1+hL) \epsilon_i + h \tau,
\tag{10.43}\]</span></span> where <span class="math display">\[
\quad \tau := \max_i \tau_i .
\]</span> (We assume here that <span class="math inline">\(\partial f  / \partial y\)</span> is a bounded function – more on this in <a href="#sec-onestep" class="quarto-xref"><span>Section 10.4</span></a>.) Finally, we apply the recursion relation <a href="#eq-epsrecur" class="quarto-xref">Eq.&nbsp;<span>10.43</span></a> iteratively, starting from the observation that <span class="math inline">\(\epsilon_0 = |\alpha-\alpha|=0\)</span>. <span class="math display">\[
\begin{aligned}
\epsilon_0 &amp;= 0,&amp; \\
\epsilon_1 &amp;\leq (1+hL) \epsilon_0  + h \tau \leq h\tau,\\
\epsilon_2 &amp;\leq (1+hL) \epsilon_1  + h \tau
    \leq ((1+hL)+1) h\tau,\\
\epsilon_3 &amp;\leq (1+hL) \epsilon_2  + h \tau
    \leq ((1+hL)^2+(1+hL)+1) h\tau,\\
&amp; \quad \vdots\notag \\
\epsilon_i &amp;\leq h \tau \sum_{k=0}^{i-1} (1+hL)^k .
\end{aligned}
\]</span> We can simplify this expression by summing the geometric sum: <span class="math display">\[
\epsilon_i \leq h \tau \frac{(1+hL)^i-1}{(1+hL)-1}
   = \frac{\tau}{L} \big((1+hL)^i-1\big).
\]</span> For further simplification, we use the inequality <span class="math inline">\(1+t \leq e^t\)</span> <span class="math inline">\((t \geq 0)\)</span> with <span class="math inline">\(t=hL\)</span> and get <span class="math display">\[
\epsilon_i \leq \frac{\tau}{L} \big(e^{ihL}-1\big) =
  \frac{\tau}{L} \big(e^{(x_i-a)L}-1\big).
\]</span> As a last step, we insert the estimate <a href="#eq-lteest" class="quarto-xref">Eq.&nbsp;<span>10.41</span></a> for the local truncation error, and obtain with <span class="math inline">\(M := \sup_{x \in [a,b]} \lvert y''(x) \rvert\)</span>,</p>
<p><span id="eq-eulergloberr"><span class="math display">\[
  \epsilon_i \leq \frac{Mh}{2L} \big(e^{(x_i-a)L}-1\big).
\tag{10.44}\]</span></span> This completes our estimate for the global error.</p>
<p><strong>Main message:</strong> Because the local truncation error of the method is <span class="math inline">\(O(h)\)</span>, the global error is also <span class="math inline">\(O(h)\)</span>. We will see later that this behaviour persists for more complicated approximation schemes – the local truncation error determines the global error, as far as the behaviour in <span class="math inline">\(h\)</span> is concerned. (Note however that the constants <span class="math inline">\(M\)</span> and <span class="math inline">\(L\)</span> depend on estimates on the derivatives of <span class="math inline">\(f\)</span>, which could be large.)</p>
<p>With the Euler method, we have an approximation algorithm for the generic initial value problem <a href="#eq-ivpeuler" class="quarto-xref">Eq.&nbsp;<span>10.4</span></a>, which works for any sufficiently smooth <span class="math inline">\(f\)</span>. We have found an explicit estimate <a href="#eq-eulergloberr" class="quarto-xref">Eq.&nbsp;<span>10.44</span></a> for the approximation error, which in particular shows that the approximation values converge to the exact solution, <span class="math inline">\(w_i \to y(x_i)\)</span> as <span class="math inline">\(h \to 0\)</span>.</p>
<p>However, the Euler method as presented here has two main shortcomings:</p>
<p>First, the convergence is rather slow - only of order <span class="math inline">\(O(h)\)</span>. One would need to choose the step size <span class="math inline">\(h\)</span> very small in order to arrive at a useful approximation. Decreasing <span class="math inline">\(h\)</span> means, first of all, an increase in computation time. But also, small values of <span class="math inline">\(h\)</span> make the difference equation <a href="#eq-eulerdiffeq" class="quarto-xref">Eq.&nbsp;<span>10.11</span></a> prone to roundoff errors; limits in floating point precision limit the usable range for <span class="math inline">\(h\)</span>. (For more details on the influence of roundoff errors on the approximation result, see for example <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010</a> Theorem 5.10)</span>.)</p>
<p>Second, we have formulated the method for a <em>first-order</em> ODE. Most applications, however, use higher-order ODEs (usually second-order), systems of first-order ODEs, or indeed a combination of both. Our approximation methods needs to be generalized to these cases in order to be useful in practice.</p>
<p>Next we will introduce the generalizations needed for handling higher-order ODEs and systems of ODEs. In most textbooks, you will find this generalization only in later chapters, for example in Sec.&nbsp;5.9 of <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010</a>)</span>, or not at all. However, here I take the viewpoint that, while treating systems of ODEs is not really difficult, it is so important that it should be introduced right in the beginning! As we shall see, this can be boiled down to almost no more than a little change in notation.</p>
</section>
<section id="systems-of-odes" class="level3" data-number="10.2.5">
<h3 data-number="10.2.5" class="anchored" data-anchor-id="systems-of-odes"><span class="header-section-number">10.2.5</span> Systems of ODEs</h3>
<p>A generic initial value problem for a system of <span class="math inline">\(m\)</span> first-order ODEs would look as follows: <span id="eq-odesysexplicit"><span class="math display">\[
\begin{aligned}
y_1\,'(x) &amp;= f_1(x,y_1(x),\ldots,y_m(x)), \\
&amp;\vdots\\
y_m\,'(x) &amp;= f_m(x,y_1(x),\ldots,y_m(x)),
\end{aligned}
\tag{10.45}\]</span></span> for <span class="math inline">\(x\)</span> in some interval <span class="math inline">\([a,b]\)</span>, with initial values <span id="eq-odesysivp"><span class="math display">\[
y_1(a) = \alpha_1, \ldots, y_m(a) = \alpha_m.
\tag{10.46}\]</span></span> Here <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(\alpha_1,\ldots,\alpha_m\)</span> are given constants, and <span class="math inline">\(f_1,\ldots,f_m\)</span> are functions from <span class="math inline">\(\mathbb{R}\times \mathbb{R}^m\)</span> to <span class="math inline">\(\mathbb{R}\)</span>. The exact solution of the system would consist of functions <span class="math inline">\(y_1,\ldots,y_m:[a,b] \to \mathbb{R}\)</span>.</p>
<p>The idea of handling these ODE systems mainly involves rewriting them in a convenient way. To that end, let us introduce the vectors <span id="eq-vecdef"><span class="math display">\[
\begin{aligned}
  \boldsymbol{\alpha}&amp;= \left(\alpha_1,\ldots,\alpha_m\right),\\
  \mathbf{y}&amp;= \left(y_1,\ldots,y_m\right),
\end{aligned}
\tag{10.47}\]</span></span> and the vector-valued function <span class="math inline">\(\mathbf{f}: \mathbb{R} \times \mathbb{R}^m \to \mathbb{R}^m\)</span> given by <span id="eq-fdef"><span class="math display">\[
\mathbf{f}(x,\mathbf{y}) = \left(f_1(x,y_1,\ldots,y_m), \ldots, f_m(x,y_1,\ldots,y_m)\right).
\tag{10.48}\]</span></span> With these, the IVP <a href="#eq-odesysexplicit" class="quarto-xref">Eq.&nbsp;<span>10.45</span></a> for the ODE system reads <span id="eq-ivpsys"><span class="math display">\[
\mathbf{y}'(x) = \mathbf{f}(x,\mathbf{y}(x)),  \quad a \leq x \leq b, \quad \mathbf{y}(a) = \boldsymbol{\alpha}.
\tag{10.49}\]</span></span></p>
<p>Note the formal similarity with the analogue <a href="#eq-ivpeuler" class="quarto-xref">Eq.&nbsp;<span>10.4</span></a> for a single ODE! Our exact solution <span class="math inline">\(\mathbf{y}\)</span> is now a function from <span class="math inline">\([a,b]\)</span> to <span class="math inline">\(\mathbb{R}^m\)</span>.</p>
<p>The idea in generalizing our approximation methods to systems of ODEs is based on this formal similarity as well. For example, the difference equation for the “Euler method for ODE systems” reads <span id="eq-eulersys"><span class="math display">\[
\begin{aligned}
   \mathbf{w}_0 &amp;:= \boldsymbol{\alpha},\\
   \mathbf{w}_{i+1} &amp;:= \mathbf{w}_i + h\, \mathbf{f}(x_i,\mathbf{w}_i) \quad (i=0,\ldots,N-1).
\end{aligned}
\tag{10.50}\]</span></span> The approximation values <span class="math inline">\(\mathbf{w}_i\)</span> are now vectors in <span class="math inline">\(\mathbb{R}^m\)</span>. The error estimates obtained in <a href="#sec-eulererror" class="quarto-xref"><span>Section 10.2.2</span></a> carry over very directly to the case of ODE systems; more on this will follow in later sections.</p>
</section>
<section id="second-order-odes" class="level3" data-number="10.2.6">
<h3 data-number="10.2.6" class="anchored" data-anchor-id="second-order-odes"><span class="header-section-number">10.2.6</span> Second-order ODEs</h3>
<p>In applications, one often meets second-order ODEs. Initial value problems for them can be defined as follows: <span id="eq-ivpsecond"><span class="math display">\[
y'' (x) = f(x,y(x),y'(x)),  \quad a \leq x \leq b, \quad y(a) = \alpha, \, y'(a)=\alpha'.
\tag{10.51}\]</span></span></p>
<p>Note that we need to specify an additional initial value <span class="math inline">\(\alpha'\in\mathbb{R}\)</span> for the first derivative.</p>
<p>Fortunately, these can be rewritten into an equivalent system of first-order ODEs, so that they are — for our purposes — not really different from what was discussed above. Namely, we substitute <span class="math inline">\(y\)</span> and <span class="math inline">\(y'\)</span> with the components of a 2-vector <span class="math inline">\(\mathbf{u}\)</span>: We set <span class="math inline">\(u_1 = y\)</span>, <span class="math inline">\(u_2=y'\)</span>.</p>
<p>More formally, this works as follows. Given a solution <span class="math inline">\(y(x)\)</span> of <a href="#eq-ivpsecond" class="quarto-xref">Eq.&nbsp;<span>10.51</span></a>, we set <span id="eq-secondsubs"><span class="math display">\[
   \mathbf{g}(x,\mathbf{u}) := \big(u_2, f(x,u_1,u_2)\big).
\tag{10.52}\]</span></span></p>
<p>It is then easy to check that <span class="math inline">\(\mathbf{u}(x)\)</span> is a solution of the IVP</p>
<p><span id="eq-ivpu"><span class="math display">\[
\mathbf{u}' = \mathbf{g}(x,\mathbf{u}),  \quad a \leq x \leq b, \quad \mathbf{u}(a) = \boldsymbol{\beta}.
\tag{10.53}\]</span></span></p>
<p>On the other hand, given a solution <span class="math inline">\(\mathbf{u}(x)\)</span> of <a href="#eq-ivpu" class="quarto-xref">Eq.&nbsp;<span>10.53</span></a>, we set <span class="math inline">\(y(x):=u_1(x)\)</span>, <span class="math inline">\(\alpha := \beta_1\)</span>, <span class="math inline">\(\alpha' := \beta_2\)</span> and obtain a solution of <a href="#eq-ivpsecond" class="quarto-xref">Eq.&nbsp;<span>10.51</span></a>. In this sense, <a href="#eq-ivpsecond" class="quarto-xref">Eq.&nbsp;<span>10.51</span></a> and <a href="#eq-ivpu" class="quarto-xref">Eq.&nbsp;<span>10.53</span></a> are equivalent; and for the purpose of developing numerical methods, it is sufficient if we consider the first-order system <a href="#eq-ivpu" class="quarto-xref">Eq.&nbsp;<span>10.53</span></a>.</p>
<div id="exm-secondorder" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.3 (Example of a second-order ODE)</strong></span> Let us illustrate the substitution process in an example. Consider the IVP for a second-order ODE, <span id="eq-ivp2ex"><span class="math display">\[
y''(x) = y'(x)\cos(x) +2 y(x),  \quad 0 \leq x \leq 1, \quad y(0) = 2, \, y'(0)=-3.
\tag{10.54}\]</span></span></p>
<p>So, <span class="math inline">\(f(x,y,y')=y'\cos(x) +2 y\)</span> in the present case. Using the rules in <a href="#eq-secondsubs" class="quarto-xref">Eq.&nbsp;<span>10.52</span></a>, we obtain an equivalent IVP for a system of two first-order equations: <span class="math display">\[
\mathbf{u}'(x) = \begin{pmatrix}
        u_2(x) \\ u_2(x)\cos(x) +2 u_1(x)           
        \end{pmatrix},
\quad 0 \leq x \leq 1, \quad \mathbf{u}(0) = \begin{pmatrix}
        \vphantom{-}2 \\ -3
        \end{pmatrix}.
\]</span> Once we have found an (approximate) solution for this system, we would set <span class="math inline">\(y(x):=u_1(x)\)</span> and obtain an (approximate) solution of <a href="#eq-ivp2ex" class="quarto-xref">Eq.&nbsp;<span>10.54</span></a>.</p>
</div>
<p>With similar methods, one can rewrite third-order, fourth-order, etc. ODEs as systems of first-order ODEs. Equally, <em>systems</em> of higher-order ODEs can be transformed into systems of first-order ODEs by appropriate substitutions. For example, systems of second-order ODEs are very common in Newtonian Mechanics: <span class="math inline">\(N\)</span> particles moving in three-dimensional space are modelled using a system of <span class="math inline">\(3N\)</span> coupled second-order ODEs, which can be transformed into a system of <span class="math inline">\(6N\)</span> coupled first-order ODEs.</p>
<p>Thus, the most general case of IVP that we need to consider is given by <a href="#eq-ivpsys" class="quarto-xref">Eq.&nbsp;<span>10.49</span></a>. In the following, we will formulate all our approximation methods for this case.</p>
</section>
</section>
<section id="sec-ivpfundam" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-ivpfundam"><span class="header-section-number">10.3</span> Fundamentals</h2>
<p>We will now take a step back, and revisit the theory of initial value problems for ODEs and their numerical treatment from a more general perspective. In doing so, we will always consider initial value problems for systems of (first-order) ODEs, in the form <a href="#eq-ivpsys" class="quarto-xref">Eq.&nbsp;<span>10.49</span></a>. This means that we will make extensive use of techniques from Vector Calculus.</p>
<section id="vectors-and-matrices" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="vectors-and-matrices"><span class="header-section-number">10.3.1</span> Vectors and matrices</h3>
<p>In order to describe numerical approximations of vectors, we will need a notion of distance between two vectors. Throughout this part of the course, we do not use the Euclidean norm for this purpose, but the <em>maximum norm</em> or <em><span class="math inline">\(\ell_\infty\)</span> norm</em>, which is given by</p>
<p><span id="eq-vectornorm"><span class="math display">\[
\lVert \mathbf{v} \rVert_\infty := \max_{1\leq i \leq m}\vert v_{i}\vert.
\tag{10.55}\]</span></span></p>
<p>From now on, we will just write <span class="math inline">\(\lVert \mathbf{v} \rVert\)</span> instead of <span class="math inline">\(\lVert \mathbf{v} \rVert_\infty\)</span>.</p>
<p>We also need a corresponding norm for matrices <span class="math inline">\(\mathbf{A}\in\mathcal{M}(m,m)\)</span>.</p>
</section>
<section id="calculus-in-several-variables" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="calculus-in-several-variables"><span class="header-section-number">10.3.2</span> Calculus in several variables</h3>
<p>We will also need to work with functions that depend on vectors, and with vector-valued functions. For a review of the techniques of Calculus in several variables, see for example <span class="citation" data-cites="Thomas:2010">(<a href="references.html#ref-Thomas:2010" role="doc-biblioref"><strong>Thomas:2010?</strong></a> Ch.&nbsp;14)</span> or <span class="citation" data-cites="Steward:1991">(<a href="references.html#ref-Steward:1991" role="doc-biblioref"><strong>Steward:1991?</strong></a> Ch.&nbsp;15)</span>. We give a very brief review here, in our notation.</p>
<p>Let <span class="math inline">\(f\)</span> be a function from <span class="math inline">\(\mathbb{R}^m\)</span> to <span class="math inline">\(\mathbb{R}\)</span> (a function of <span class="math inline">\(m\)</span> variables). Instead of derivatives of functions of a single variable, one can consider <em>partial derivatives</em> of <span class="math inline">\(f\)</span>, denoted as <span class="math display">\[
\frac{\partial f}{\partial x_j}(\mathbf{x}) = \frac{d}{dt} f(x_1, \ldots, x_j+t, \ldots,  x_m) \Big\vert_{t=0},
\]</span> and higher order partial derivatives accordingly. If <span class="math inline">\(\mathbf{f}\)</span> is a vector-valued function, that is, <span class="math inline">\(\mathbf{f}: \mathbb{R}^m \to \mathbb{R}^k\)</span>, then all partial derivatives are vector-valued as well (each component is differentiated). All first derivatives of such a function can conveniently be combined into an <span class="math inline">\(k\times m\)</span> matrix, <span class="math display">\[
\frac{\partial \mathbf{f}}{\partial \mathbf{x}} =  \begin{pmatrix}
        \frac{\partial f_1}{\partial x_1} &amp;\cdots&amp; \frac{\partial f_1}{\partial x_m} \\
         \vdots &amp; &amp; \vdots \\
        \frac{\partial f ^{(k)}}{\partial x_1} &amp;\cdots&amp; \frac{\partial f ^{(k)}}{\partial x_m}
       \end{pmatrix}
\]</span> The multi-dimensional chain rule can be expressed quite easily in this formalism: if <span class="math inline">\(\mathbf{f}\)</span> and <span class="math inline">\(\mathbf{g}\)</span> are vector-valued mappings such that <span class="math inline">\(\mathbf{g}\circ \mathbf{f}: \mathbf{x}\mapsto \mathbf{g}(\mathbf{f}(\mathbf{x}))\)</span> is defined (i.e.&nbsp;the range of <span class="math inline">\(\mathbf{f}\)</span> matches the domain of <span class="math inline">\(\mathbf{g}\)</span>) then the derivative of <span class="math inline">\(\mathbf{g}\circ \mathbf{f}\)</span> is given by <span class="math display">\[
\frac{\partial (\mathbf{g}\circ \mathbf{f})}{\partial \mathbf{x}}
=  \frac{\partial \mathbf{g}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{f}}{\partial \mathbf{x}}  .
\]</span> where <span class="math inline">\(\cdot\)</span> represents matrix multiplication. Higher-order derivatives can be treated in a similar, though somewhat more complicated matrix formalism.</p>
<p>Also, a generalization of Taylor’s theorem holds for functions of several variables. This is summarized in Appendix&nbsp;<a href="appendix.html#app:taylorthm" data-reference-type="ref" data-reference="app:taylorthm">4</a>.</p>
</section>
<section id="odes" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="odes"><span class="header-section-number">10.3.3</span> ODEs</h3>
<p>We now treat initial value problems for ODEs in our vector formalism. In a first reading, it is always a useful exercise to reduce our statements to the case of one ODE (<span class="math inline">\(m=1\)</span>), in which case the computations become more elementary. In this case, we can simply replace <span class="math inline">\(\mathbf{y}\)</span> with a scalar function <span class="math inline">\(y\)</span>, the initial value vector <span class="math inline">\(\boldsymbol{\alpha}\)</span> with a number <span class="math inline">\(\alpha\)</span>, the norm <span class="math inline">\(\lVert  \, \cdot \,  \rVert\)</span> with the absolute value <span class="math inline">\(\lvert \, \cdot \, \rvert\)</span>, and so forth.</p>
<p>Let us first define formally what we mean by a solution of an initial value problem.</p>
<div id="def-ivp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.1</strong></span> Let <span class="math inline">\(m \in \mathbb{N}\)</span>, <span class="math inline">\(a&lt;b \in \mathbb{R}\)</span>, <span class="math inline">\(\boldsymbol{\alpha}\in \mathbb{R}^m\)</span>, and <span class="math inline">\(\mathbf{f}: [a,b]\times \mathbb{R}^m \to \mathbb{R}^m\)</span>. We say that a function <span class="math inline">\(\mathbf{y}\in\mathcal{C}^1([a,b],\mathbb{R}^m)\)</span> is a <strong>solution of the initial value problem (IVP)</strong> <span id="eq-ivpformaldef"><span class="math display">\[
\mathbf{y}' = \mathbf{f}(x,\mathbf{y}),  \quad a \leq x \leq b, \quad \mathbf{y}(a) = \boldsymbol{\alpha}
\tag{10.56}\]</span></span></p>
<p>if for all <span class="math inline">\(x \in [a,b]\)</span>, <span id="eq-ivpformaldef2"><span class="math display">\[
\mathbf{y}'(x) = \mathbf{f}(x,\mathbf{y}(x)) \quad \text{and}  \quad \mathbf{y}(a)=\boldsymbol{\alpha}.
\tag{10.57}\]</span></span></p>
</div>
<p>Our first question is when such an IVP has a unique solution (so that we can reasonably search for a numerical approximation of it). The key condition involved here is the so-called <em>Lipschitz condition</em>.</p>
<div id="def-lipcond" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 10.2</strong></span> We say that <span class="math inline">\(\mathbf{f}:[a,b]\times \mathbb{R}^m \to \mathbb{R}^m\)</span> satisfies a <strong>Lipschitz condition</strong> if there is a constant <span class="math inline">\(L&gt;0\)</span> such that for all <span class="math inline">\(x \in [a,b]\)</span> and <span class="math inline">\(\mathbf{y},\hat{\mathbf{y}}\in\mathbb{R}^m\)</span>,</p>
<p><span id="eq-lipschitzdef"><span class="math display">\[
  \lVert  \mathbf{f}(x,\mathbf{y}) - \mathbf{f}(x,\hat{\mathbf{y}})  \rVert \leq L \| \mathbf{y}-\hat{\mathbf{y}}\|.
\tag{10.58}\]</span></span> The constant <span class="math inline">\(L\)</span> above is called a <strong>Lipschitz constant</strong>.</p>
</div>
<p>The Lipschitz condition is in a sense an intermediate concept between continuity and differentiability. Often, we can use a simple criterion to check it — in fact, we already have, in the definition of the constant <span class="math inline">\(L\)</span> in the error analysis of Euler’s method in one variable.</p>
<div id="lem-lipderiv" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 10.1</strong></span> If <span class="math inline">\(\partial \mathbf{f}/ \partial \mathbf{y}\)</span> exists and is bounded, then <span class="math inline">\(\mathbf{f}\)</span> satisfies a Lipschitz condition with Lipschitz constant <span id="eq-lipschitzderiv"><span class="math display">\[
L = \sup\left\{ \Big\lVert \frac{\partial \mathbf{f}}{\partial \mathbf{y}} (x,\mathbf{y}) \Big\rVert  : x \in [a,b], \mathbf{y}\in \mathbb{R}^m \right\}.
\tag{10.59}\]</span></span> *</p>
</div>
<p>Note here that <span class="math inline">\(\partial \mathbf{f}/ \partial \mathbf{y}\)</span> is an <span class="math inline">\(m \times m\)</span> <em>matrix</em>! Again, you may first want to consider the case <span class="math inline">\(m=1\)</span>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Given <span class="math inline">\(x \in [a,b]\)</span>, <span class="math inline">\(\mathbf{y},\hat{\mathbf{y}}\in \mathbb{R}^m\)</span>, and <span class="math inline">\(j \in \{1,\ldots, m\}\)</span>, consider the function <span class="math inline">\(g:[0,1]\to\mathbb{R}\)</span> defined by <span class="math inline">\(g(t)=f_j((1-t)\hat{\mathbf{y}}+t\mathbf{y})\)</span>, so <span class="math inline">\(g(0)=\hat{\mathbf{y}}\)</span> and <span class="math inline">\(g(1)=\mathbf{y}\)</span>, and apply the scalar MVT to give <span class="math inline">\(\mathbf{y}-\hat{\mathbf{y}}=g(1)-g(0)=g'(t_0)\)</span> for some <span class="math inline">\(t_0\in(0,1)\)</span>; now, using the chain rule, <span class="math display">\[
g'(t)=\frac{\partial f_j}{ \partial \mathbf{y}}(x,(1-t)\hat{\mathbf{y}}+t\mathbf{y})\cdot\frac{d}{dt}((1-t)\hat{\mathbf{y}}+t\mathbf{y})=\frac{\partial f_j}{ \partial \mathbf{y}}(x,(1-t)\hat{\mathbf{y}}+t\mathbf{y})\cdot(\mathbf{y}-\hat{\mathbf{y}}).
\]</span> Combining these gives us <span class="math display">\[
f_j(x,\mathbf{y}) - f_j(x,\hat{\mathbf{y}}) =
\frac{\partial f_j}{ \partial \mathbf{y}}(x,\boldsymbol{\eta}) \cdot (\mathbf{y}-\hat{\mathbf{y}})
\]</span> where <span class="math inline">\(\boldsymbol{\eta}=(1-t_0)\hat{\mathbf{y}}+t_0\mathbf{y}\)</span>. Using the property of the vector norm of the scalar product (see <a href="chapter04.html#def-vecnorm" class="quarto-xref">Definition&nbsp;<span>4.1</span></a>) we find <span class="math display">\[
| f_j(x,\mathbf{y}) - f_j(x,\hat{\mathbf{y}}) | \leq  
   \Big\lVert \frac{\partial f_j}{ \partial \mathbf{y}} (x,\boldsymbol{\eta}) \Big\rVert
\, \| \mathbf{y}-\hat{\mathbf{y}}\|
    \leq L \| \mathbf{y}-\hat{\mathbf{y}}\|
\]</span> with <span class="math inline">\(L\)</span> as in <a href="#eq-lipschitzderiv" class="quarto-xref">Eq.&nbsp;<span>10.59</span></a>. Taking the maximum over <span class="math inline">\(j\)</span> implies the result.&nbsp;◻</p>
</div>
<p>Why do we consider this kind of condition? Because the Lipschitz condition is the essential ingredient for the well-definedness of the initial value problem. Namely, one has:</p>
<div id="thm-ivpunique" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.1</strong></span> Let <span class="math inline">\(\mathbf{f}: [a,b]\times \mathbb{R}^m \to \mathbb{R}^m\)</span> be continuous and satisfy a Lipschitz condition. Then, for any <span class="math inline">\(\boldsymbol{\alpha}\in \mathbb{R}^m\)</span>, the initial value problem <a href="#eq-ivpformaldef" class="quarto-xref">Eq.&nbsp;<span>10.56</span></a> has a unique solution <span class="math inline">\(\mathbf{y}\)</span>.</p>
</div>
<p>This theorem is given here without proof. (It is one of the central theorems in the theory of ordinary differential equations. See <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010</a> Theorem&nbsp;5.17)</span> for references.)</p>
<p>The Lipschitz condition is not only relevant for the abstract existence of a solution. Recalling Eqs.&nbsp;<a href="#eq-eulerlipstart" class="quarto-xref">Eq.&nbsp;<span>10.42</span></a>–<a href="#eq-ldef" class="quarto-xref">Eq.&nbsp;<span>10.31</span></a>, we see that it also enters our error estimates for numeric approximations. We will analyse this in more detail later.</p>
<p>Let us discuss a few examples for <span class="math inline">\(m=1\)</span>, on the interval <span class="math inline">\([a,b]=[0,2]\)</span>.</p>
<div id="exm-unique" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.4 (Unique solution)</strong></span> Let us recall the example <a href="#eq-eulerivpexample" class="quarto-xref">Eq.&nbsp;<span>10.12</span></a>, with <span class="math inline">\(f(x,y)=y-x^2+1\)</span>. This function is certainly continuous in both variables, and differentiable in <span class="math inline">\(y\)</span>. We have <span class="math inline">\(\partial f(x,y) / \partial y = 1\)</span>; in particular, the derivative is bounded. By <a href="#lem-lipderiv" class="quarto-xref">Lemma&nbsp;<span>10.1</span></a>, we have a Lipschitz constant <span class="math inline">\(L=1\)</span> and thus, by <a href="#thm-ivpunique" class="quarto-xref">Theorem&nbsp;<span>10.1</span></a>, a unique solution of the IVP. In fact, for the initial condition <span class="math inline">\(y(0)=1/2\)</span>, the solution is given in <a href="#eq-eulerexact" class="quarto-xref">Eq.&nbsp;<span>10.13</span></a>.</p>
</div>
<div id="exm-nosolution" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.5 (No solution)</strong></span> Consider <span class="math inline">\(f(x,y)=y^2+1\)</span>. Again, the function is continuous and differentiable. However, <span class="math inline">\(\partial f(x,y) / \partial y = 2y\)</span> is unbounded, so <a href="#lem-lipderiv" class="quarto-xref">Lemma&nbsp;<span>10.1</span></a> cannot be applied. Actually, we can explicitly see that <span class="math inline">\(f\)</span> does <em>not</em> satisfy a Lipschitz condition. Namely, if it did, then the quotient <span class="math display">\[
\frac{|f(x,y)-f(x,\hat{y})|}{|y-\hat{y}|}         \quad (\text{for }x \in [0,2],\; y \neq \hat{y} \in \mathbb{R})
\]</span> would be bounded (by the Lipschitz constant <span class="math inline">\(L\)</span>). However, in our case, set <span class="math inline">\(\hat{y}=0\)</span>; then <span class="math display">\[
\frac{|f(x,y)-f(x,\hat{y})|}{|y-\hat{y}|}
= \frac{|y^2 +1-1|}{|y|} = |y|
\]</span> which is <em>not</em> bounded. So <span class="math inline">\(f\)</span> cannot satisfy a Lipschitz condition.</p>
<p>Hence, for the corresponding initial value problem, <span class="math display">\[
y' = y^2+1,  \quad 0 \leq x \leq 2, \quad y(0) = 0,
\]</span> the existence and uniqueness result <a href="#thm-ivpunique" class="quarto-xref">Theorem&nbsp;<span>10.1</span></a> does not apply. In fact, a solution for small <span class="math inline">\(x\)</span> is given by <span class="math display">\[
y(x) = \tan(x),
\]</span> but this solution does <em>not</em> exist for all <span class="math inline">\(x \in [0,2]\)</span>.</p>
<p>The problem here is that <span class="math inline">\(\partial f(x,y) / \partial y\)</span> is unbounded as <span class="math inline">\(y\)</span> grows large. One can still control IVPs of this kind, using so-called <em>local Lipschitz conditions</em>, where the estimate <a href="#eq-lipschitzdef" class="quarto-xref">Eq.&nbsp;<span>10.58</span></a> is required to hold only for <span class="math inline">\((x,\mathbf{y}),(x,\hat{\mathbf{y}})\in \mathcal{D}\)</span> with <span class="math inline">\(\mathcal{D}\)</span> any fixed compact set, and the Lipschitz constant <span class="math inline">\(L\)</span> is allowed to depend on <span class="math inline">\(\mathcal{D}\)</span>. Our numerical methods will still be applicable in this case, as long as we do not approach the possible singularities of the solution too closely. However, the formalism becomes much more complicated, and we do not treat these cases explicitly here.</p>
</div>
<div id="exm-nonunique" class="theorem example">
<p><span class="theorem-title"><strong>Example 10.6 (Non-unique solution)</strong></span> Consider <span class="math inline">\(f(x,y)=\sqrt[3]{y}\)</span>. While <span class="math inline">\(f\)</span> is continuous, the partial derivative <span class="math inline">\(\partial f(x,y) / \partial y\)</span> does <em>not exist</em> at <span class="math inline">\(y=0\)</span>, so again, <a href="#lem-lipderiv" class="quarto-xref">Lemma&nbsp;<span>10.1</span></a> cannot be applied. Indeed, set <span class="math inline">\(\hat{y}=0\)</span>, then <span class="math display">\[
\frac{|f(x,y)-f(x,\hat{y})|}{|y-\hat{y}|}
= \frac{|y|^{1/3}}{|y|} = |y|^{-2/3}
\]</span> which is unbounded near <span class="math inline">\(y=0\)</span>. So <span class="math inline">\(f\)</span> does not satisfy a Lipschitz condition, and we are not guaranteed a unique solution of the IVP <span id="eq-badivp"><span class="math display">\[
y' = \sqrt[3]{y},  \quad 0 \leq x \leq 2, \quad y(0) = 0.
\tag{10.60}\]</span></span> In fact, this IVP has at least <em>three</em> solutions: <span class="math display">\[
y(x)= \Big(\frac{2x}{3}\Big)^{3/2},\quad
y(x)= -\Big(\frac{2x}{3}\Big)^{3/2},\quad
y(x) = 0.
\]</span></p>
<p>This case is, in a sense, worse than the other examples above. Running our numerical methods for the IVP <a href="#eq-badivp" class="quarto-xref">Eq.&nbsp;<span>10.60</span></a> is likely to give unpredictable results; even a small rounding error might cause us to “switch” between the different solutions of the IVP.</p>
</div>
</section>
</section>
<section id="sec-onestep" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="sec-onestep"><span class="header-section-number">10.4</span> One-Step Difference Methods</h2>
<p>We now return to the initial value problem <a href="#eq-ivpformaldef" class="quarto-xref">Eq.&nbsp;<span>10.56</span></a>: <span class="math display">\[
\mathbf{y}'(x) = \mathbf{f}(x,\mathbf{y}(x)),  \quad a \leq x \leq b, \quad \mathbf{y}(a) = \boldsymbol{\alpha}.
\]</span> As in Euler’s method, we use equally-spaced mesh points <span class="math inline">\(x_0,\ldots,x_N\)</span> in the interval <span class="math inline">\([a,b]\)</span>, with a uniform step size <span class="math inline">\(h=(b-a)/N\)</span>, so <span class="math inline">\(x_i=a+ih\)</span>, and we seek approximate values <span class="math inline">\(\mathbf{w}_i\in\mathbb{R}^m\)</span>, with <span class="math inline">\(\mathbf{w}_i\approx\mathbf{y}(x_i)\)</span>.</p>
<p>Euler’s method was based on the Taylor expansion <span class="math display">\[
\underbrace{y(x+h)}_\text{exact}=\underbrace{y(x)+hy'(x)}_{\text{approx}}+\underbrace{\frac{h^2}{2}y''(\xi)}_{\text{error}}.
\]</span> Any other formula of this type can be used to define a similar method and, if the error term is smaller than the one in Euler’s method, should lead to smaller errors in the approximate solution. In complete generality, and for vectors instead of scalars, we consider a formula <span class="math display">\[
\mathbf{y}(x+h)=\mathbf{y}(x)+h\boldsymbol{\phi}(x,\mathbf{y}(x),h)+\text{error}
\]</span> All the higher-order Taylor approximations fit in to this framework, as do the important Runge-Kutta methods described below. This leads to the general single-step method <span id="eq-onestepdef"><span class="math display">\[
\begin{aligned}
  \mathbf{w}_0 &amp;:= \mathbf{\alpha},
\\
  \mathbf{w}_{i+1} &amp;:= \mathbf{w}_i + h \boldsymbol{\phi}(x_i,\mathbf{w}_i,h).
\end{aligned}
\tag{10.61}\]</span></span> Introducing some notation for the error, we write <span class="math display">\[
\mathbf{y}(x_{i+1})=\mathbf{y}(x_i)+h\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)+h\boldsymbol{\tau}_{i+1}(h)
\]</span> where <span class="math inline">\(\boldsymbol{\tau}_{i+1}(h)\)</span>, the <em>local truncation error</em>, is defined by <span id="eq-lteonestep"><span class="math display">\[
\boldsymbol{\tau}_{i+1}(h)=\frac{1}{h}\left[\mathbf{y}(x_{i+1})-\mathbf{y}(x_i)-h\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)\right]
\tag{10.62}\]</span></span></p>
<p>In terms of the numerical method, this is the error that would occur at step <span class="math inline">\(i+1\)</span> if we started on the exact solution curve, divided by the step size. We assume we have an upper bound <span class="math inline">\(\tau(h)\)</span> such that, for some <span class="math inline">\(h_0&gt;0\)</span>, <span class="math display">\[
\|\boldsymbol{\tau}_i(h)\|\leq\tau(h)
\]</span> for all <span class="math inline">\(i\in\{0,\ldots,N\}\)</span> and all <span class="math inline">\(h\in(0,h_0]\)</span>. For the method to be useful, we must have <span class="math inline">\(\tau(h)\to 0\)</span> as <span class="math inline">\(h\to 0+\)</span>; in Euler’s method, <span class="math inline">\(\tau(h)=Mh/2\)</span> but in other methods <span class="math inline">\(\tau(h)\)</span> is much smaller than this: typically <span class="math inline">\(\tau(h)=O(h^n)\)</span> as <span class="math inline">\(h\to 0+\)</span> for some <span class="math inline">\(n&gt;1\)</span>. We also assume the existence of a Lipschitz constant <span class="math inline">\(L\)</span> such that <span class="math display">\[
\|\boldsymbol{\phi}(x, \mathbf{y}, h)-\boldsymbol{\phi}(x, \hat{\mathbf{y}}, h)|\leq L\|\mathbf{y}-\hat{\mathbf{y}}\|
\]</span> for all <span class="math inline">\(x\in[a,b]\)</span>, for all <span class="math inline">\(\mathbf{y},\hat{\mathbf{y}}\in\mathbb{R}^m\)</span> and all <span class="math inline">\(h\in(0,h_0]\)</span> (this replaces the second use of Taylor’s Theorem in the analysis of Euler’s method; looking back at that calculation, the Lipschitz constant is all that was needed). We conclude the setup by defining the actual error at step <span class="math inline">\(i\)</span> by <span class="math display">\[
\boldsymbol{\varepsilon}_i=\mathbf{y}(x_i)-\mathbf{w}_i.
\]</span></p>
<p>Under these hypotheses, we have:</p>
<div id="thm-globalerr" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.2</strong></span> Consider an initial value problem as in <a href="#eq-ivpformaldef" class="quarto-xref">Eq.&nbsp;<span>10.56</span></a>, which we assume to have a solution <span class="math inline">\(\mathbf{y}\)</span>. For this IVP, consider the one-step difference method described by <a href="#eq-onestepdef" class="quarto-xref">Eq.&nbsp;<span>10.61</span></a>. Then, <span class="math display">\[
\lVert \mathbf{y}(x_i) - \mathbf{w}_i \rVert \leq \frac{\tau(h)}{L} \big(e^{L(x_i-a)}-1\big)
\quad \text{for all } h\in(0,h_0], \; i \in \{0,\ldots,N\}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We establish this in much the same way as we did the corresponding result for Euler’s method. Starting with the Taylor-like formula for <span class="math inline">\(\mathbf{y}(x_{i+1})\)</span> in terms of <span class="math inline">\(\mathbf{y}(x_i)\)</span>, <span class="math inline">\(\boldsymbol{\phi}\)</span> and <span class="math inline">\(\boldsymbol{\tau}_{i+1}\)</span>, we have: <span class="math display">\[
\begin{aligned}
\mathbf{y}(x_{i+1}) &amp;= \mathbf{y}(x_i+h) \\
             &amp;= \mathbf{y}(x_i)+h\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)+h\boldsymbol{\tau}_{i+1}(h) \\
             &amp;= \boldsymbol{\varepsilon}_i+\underbrace{\mathbf{w}_i+h\boldsymbol{\phi}(x_i,\mathbf{w}_i,h)}_{=\mathbf{w}_{i+1}}+h[\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)-\boldsymbol{\phi}(x_i,\mathbf{w}_i,h)]+h\boldsymbol{\tau}_{i+1}(h) \\
           &amp;= \boldsymbol{\varepsilon}_i+\mathbf{w}_{i+1}+h[\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)-\boldsymbol{\phi}(x_i,\mathbf{w}_i,h)]+h\boldsymbol{\tau}_{i+1}(h) \\
\end{aligned}
\]</span> so, subtracting <span class="math inline">\(\mathbf{w}_{i+1}\)</span> from both sides, <span class="math display">\[
\boldsymbol{\varepsilon}_{i+1}=\boldsymbol{\varepsilon}_i+h[\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)-\boldsymbol{\phi}(x_i,\mathbf{w}_i,h)]+h\boldsymbol{\tau}_{i+1}(h)
\]</span> Exactly as for Euler’s method, we estimate using the triangle inequality <span class="math display">\[
\|\boldsymbol{\varepsilon}_{i+1}\|\leq\|\boldsymbol{\varepsilon}_i\|+h\|\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)-\boldsymbol{\phi}(x_i,\mathbf{w}_i,h)\|+\|\boldsymbol{\tau}_{i+1}(h)\|\leq(1+hL)\|\boldsymbol{\varepsilon}_i\|+h\tau(h)
\]</span> using the Lipschitz hypothesis on <span class="math inline">\(\boldsymbol{\phi}\)</span> and the upper bound <span class="math inline">\(\tau\)</span> for the local truncation errors. This is exactly the same formula as for Euler, so we can read off the same answer: <span class="math display">\[
\|\boldsymbol{\varepsilon}_n\|\leq\frac{\tau(h)}{L}(e^{L(x_n-a)}-1)\leq\frac{\tau(h)}{L}(e^{L(b-a)}-1)
\]</span></p>
</div>
<p>The main conclusion is that if we leave the integration width fixed and decrease the step size, then the error at each point is bounded above by a fixed multiple of <span class="math inline">\(\tau(h)\)</span>: local error determines global error.</p>
<p>We are now aiming at refinements or modifications of Euler’s Method that are of higher order in <span class="math inline">\(h\)</span>. The class we will first look at is the so-called <em>one-step difference methods</em>.</p>
<p>Again, we start from the initial value problem <span class="math display">\[
\mathbf{y}' = \mathbf{f}(x,\mathbf{y}),  \quad a \leq x \leq b, \quad \mathbf{y}(a) = \boldsymbol{\alpha}.
\]</span> As before, we use equally-spaced mesh points <span class="math inline">\(x_0,\ldots,x_N\)</span> in the interval <span class="math inline">\([a,b]\)</span>, with a uniform step size <span class="math inline">\(h\)</span>. Approximation values <span class="math inline">\(\mathbf{w}_i\in\mathbb{R}^m\)</span> for <span class="math inline">\(\mathbf{y}(x_i)\)</span> are defined recursively by a <em>difference equation</em>, <span class="math display">\[
\begin{aligned}
  \mathbf{w}_0 &amp;:= \mathbf{\alpha},
\\
  \mathbf{w}_{i+1} &amp;:= \mathbf{w}_i + h \boldsymbol{\phi}(x_i,\mathbf{w}_i,h).
\end{aligned}
\]</span> The function <span class="math inline">\(\boldsymbol{\phi}: [a,b]\times \mathbb{R}^m \times \mathbb{R}_+\to\mathbb{R}\)</span> defines the difference method. In the case of Euler’s method, we had <span class="math inline">\(\boldsymbol{\phi}(t,y,h)=f(t,y)\)</span>, but in general we will allow <span class="math inline">\(\boldsymbol{\phi}\)</span> to depend on the step size <span class="math inline">\(h\)</span> as well. We will see several examples of such one-step methods, with different ways of obtaining the function <span class="math inline">\(\boldsymbol{\phi}\)</span>. However, let us first look at some general aspects.</p>
<p>In generalization of Eq.&nbsp;<a href="#eq-lteeuler" class="quarto-xref">Eq.&nbsp;<span>10.39</span></a> above, we define the local truncation error <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span id="eq-lteonestep"><span class="math display">\[
  \tau_{i+1} := \frac{1}{h} \big\lVert \mathbf{y}(x_{i+1}) - \mathbf{y}(x_{i}) - h \,\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h) \big\rVert.
\tag{10.63}\]</span></span></p>
<p>Given a concrete function <span class="math inline">\(\boldsymbol{\phi}\)</span>, we will want to obtain upper bounds for <span class="math inline">\(\tau_i\)</span>, depending on <span class="math inline">\(h\)</span>. In particular, we are interested in choices for <span class="math inline">\(\boldsymbol{\phi}\)</span> such that the bounds behave like <span class="math inline">\(O(h^n)\)</span> with <span class="math inline">\(n &gt; 1\)</span>.</p>
<p>Like in the Euler method, the local truncation error determines the global error.</p>
<div id="thm-globalerr" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.3</strong></span> Consider an initial value problem as in <a href="#eq-ivpformaldef" class="quarto-xref">Eq.&nbsp;<span>10.56</span></a>, which we assume to have a unique solution <span class="math inline">\(\mathbf{y}(x)\)</span>. For this IVP, consider a one-step difference method as in <a href="#eq-onestepdef" class="quarto-xref">Eq.&nbsp;<span>10.61</span></a>. Let <span class="math inline">\(h_0&gt;0\)</span> be fixed, and let <span class="math inline">\(\tau : (0,h_0] \to \mathbb{R}_+\)</span> such that the local truncation error of the method is bounded by <span class="math display">\[
\tau_{i} \leq \tau(h) \quad \text{for all } 0 &lt; h \leq h_0, \; i \in \{1,\ldots,N\}.
\]</span> Suppose further that at fixed <span class="math inline">\(h \in (0,h_0]\)</span>, the function <span class="math inline">\(\boldsymbol{\phi}( \, \cdot \, , \, \cdot \, ,h)\)</span> satisfies a Lipschitz condition, with Lipschitz constant <span class="math inline">\(L\)</span> which does not depend on <span class="math inline">\(h\)</span>. Then, <span class="math display">\[
\lVert \mathbf{y}(x_i) - \mathbf{w}_i \rVert \leq \frac{\tau(h)}{L} \big(e^{L(x_i-a)}-1\big)
\quad \text{for all } 0 &lt; h \leq h_0, \; i \in \{0,\ldots,N\}.
\]</span></p>
</div>
<p>We prove this essentially by the same methods as used in <a href="#sec-euler" class="quarto-xref"><span>Section 10.2</span></a>, but in a somewhat cleaned-up version.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For <span class="math inline">\(i \in \{0,\ldots,N\}\)</span>, define <span class="math display">\[
\epsilon_i := \lVert  \mathbf{y}(x_{i})-\mathbf{w}_i   \rVert.
\]</span> By induction on <span class="math inline">\(i\)</span>, we will prove the following estimate for <span class="math inline">\(\epsilon_i\)</span>:</p>
<p><span id="eq-epsinduct"><span class="math display">\[
  \epsilon_i \leq h \tau(h) \sum_{k=0}^{i-1} (1+hL)^k .
\tag{10.64}\]</span></span> Namely, for <span class="math inline">\(i=0\)</span> this is trivial, since <span class="math inline">\(\epsilon_0=0\)</span>. Suppose now that <a href="#eq-epsinduct" class="quarto-xref">Eq.&nbsp;<span>10.64</span></a> is already proven for some <span class="math inline">\(i\)</span>, we will prove it for <span class="math inline">\(i+1\)</span>. The triangle inequality yields <span class="math display">\[
\epsilon_{i+1} \leq \epsilon_i + \lVert  \mathbf{y}(x_{i+1}) - \mathbf{w}_{i+1} - \mathbf{y}(x_i) + \mathbf{w}_i  \rVert.
\]</span> Using the difference equation <a href="#eq-onestepdef" class="quarto-xref">Eq.&nbsp;<span>10.61</span></a> and again the triangle inequality, we have <span class="math display">\[
\epsilon_{i+1} \leq \epsilon_i + \lVert  \mathbf{y}(x_{i+1}) - \mathbf{y}(x_i) - h\boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h)  \rVert
   + h \lVert  \boldsymbol{\phi}(x_i,\mathbf{y}(x_i),h) - \boldsymbol{\phi}(x_i,\mathbf{w}_i,h) \rVert.
\]</span> Here the second term is estimated by <span class="math inline">\(h\tau(h)\)</span>, and the third term by the Lipschitz condition: <span class="math display">\[
\epsilon_{i+1} \leq \epsilon_i + h \tau(h) + h L \underbrace{\lVert  \mathbf{y}(x_i)-\mathbf{w}_i \rVert}_{\epsilon_i}
   = h \tau(h) + (1+hL) \epsilon_i.
\]</span> We insert the induction hypothesis <a href="#eq-epsinduct" class="quarto-xref">Eq.&nbsp;<span>10.64</span></a> and find, <span class="math display">\[
\epsilon_{i+1} \leq  h \tau(h) \Big( 1 + (1+h L)\sum_{k=0}^{i-1} (1+hL)^k \Big)
= h \tau(h) \sum_{k=0}^{i} (1+hL)^k .
\]</span> This proves <a href="#eq-epsinduct" class="quarto-xref">Eq.&nbsp;<span>10.64</span></a>.—Solving the geometric sum, <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and using <span class="math inline">\((1+t) \leq e^t\)</span> with <span class="math inline">\(t=hL\)</span>, this gives <span class="math display">\[
\epsilon_i \leq \frac{\tau(h)}{L} \big( (1+hL)^i-1\big)
   \leq \frac{\tau(h)}{L} \big( e^{ihL}-1 \big)
   = \frac{\tau(h)}{L} \big( e^{L(x_i-a)}-1 \big).
\]</span> This is the proposed estimate.&nbsp;◻</p>
</div>
<p>Thus, also in the general case, if the local truncation order is of order <span class="math inline">\(O(h^n)\)</span>, then the the global error is of the same order. In the following, we will see a variety of one-step methods which aim at achieving a low local truncation error. We will then only need to prove an estimate for <span class="math inline">\(\tau_{i}\)</span> as defined in <a href="#eq-lteonestep" class="quarto-xref">Eq.&nbsp;<span>10.63</span></a>, and the behaviour of the global error with <span class="math inline">\(h\)</span> will be under control.</p>
</section>
<section id="taylor-methods" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="taylor-methods"><span class="header-section-number">10.5</span> Taylor Methods</h2>
<p>We will now see the first example of one-step difference methods (beyond Euler’s method), namely the so-called <em>Taylor Methods</em>.</p>
<p>The idea here is as follows. In Euler’s Method, we approximated the exact solution <span class="math inline">\(y(x)\)</span> with a linear function, as in <a href="#eq-eulertaylor" class="quarto-xref">Eq.&nbsp;<span>10.40</span></a>. In other words, we used a first-order Taylor expansion. For a better approximation, we can use an <span class="math inline">\(n\)</span>-th order Taylor expansion, now for a vector-valued <span class="math inline">\(\mathbf{y}(x)\)</span>:</p>
<p><span id="eq-ytaylorn"><span class="math display">\[
+ \frac{h^n}{n!}  \frac{ \mathrm{d}^{n} \mathbf{y} }{\mathrm{d}x^{n} }  (x_i) + \mathbf{R}_i,
\tag{10.65}\]</span></span></p>
<p>where according to <span class="quarto-unresolved-ref">?thm-taylor1nd</span>, the remainder term <span class="math inline">\(\mathbf{R}_i\)</span> is bounded by <span id="eq-tmremainder"><span class="math display">\[
\Big\lVert  \frac{ \mathrm{d}^{n+1} \mathbf{y} }{\mathrm{d}x^{n+1} }  (x) \Big\rVert.
\tag{10.66}\]</span></span></p>
<p>Following our approach in the Euler method, we now need to replace the unknown exact solution <span class="math inline">\(\mathbf{y}(x)\)</span> in the right-hand side of <a href="#eq-ytaylorn" class="quarto-xref">Eq.&nbsp;<span>10.65</span></a> with expressions in the function <span class="math inline">\(\mathbf{f}\)</span>. Since <span class="math inline">\(\mathbf{y}(x)\)</span> solves the ODE, we can certainly write</p>
<p><span id="eq-firstderivative"><span class="math display">\[
    \frac{ \mathrm{d}^{} \mathbf{y} }{\mathrm{d}x^{} } (x) = \mathbf{f}(x,\mathbf{y}(x)),
\tag{10.67}\]</span></span></p>
<p>To obtain the second derivative, we need to differentiate this, which requires the use of the chain rule At this point, we <em>need</em> to use the chain rule in several variables: Even if <span class="math inline">\(m=1\)</span>, the function <span class="math inline">\(f\)</span> depends on two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. One of the functions we need to consider (the outermost function) is the mapping from <span class="math inline">\(\mathbb{R}^{m+1}\)</span> to <span class="math inline">\(\mathbb{R}^m\)</span> <span class="math display">\[
(x,y_1,\dots,y_m\mapsto \mathbf{f}(x,\mathbf{y})
\]</span> The derivative of this is the <span class="math inline">\(m\times(m+1)\)</span> matrix given by</p>
<p><span id="eq-outer"><span class="math display">\[
\left[\frac{\partial\mathbf{f}(x,\mathbf{y})}{\partial x}, \frac{\partial\mathbf{f}(x,\mathbf{y})}{\partial \mathbf{y}}\right]
\tag{10.68}\]</span></span></p>
<p>which represents the <span class="math inline">\(m\times 1\)</span> column vector <span class="math inline">\(\partial\mathbf{f}(x,\mathbf{y})/\partial x\)</span> being concatenated horizontally with the <span class="math inline">\(m\times m\)</span> matrix <span class="math inline">\(\partial\mathbf{f}(x,\mathbf{y})/\partial \mathbf{y}\)</span>. The other function we need to consider (the innermost function) is the mapping from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}^{m+1}\)</span> <span class="math display">\[
x\mapsto[x,\mathbf{y}(x)]:=(x,y_1(x),\dots,y_m(x))
\]</span> whose derivative is the <span class="math inline">\(m+1\)</span> dimensional column vector <span id="eq-inner"><span class="math display">\[
\end{bmatrix}
\tag{10.69}\]</span></span> where we have substituted for <span class="math inline">\(\mathbf{y}'(x)\)</span> using the differential equation. Here we have stacked the scalar <span class="math inline">\(1\)</span> on top of the <span class="math inline">\(m\)</span>-dimensional column vector <span class="math inline">\(\mathbf{y}'(x)=\mathbf{f}(x,\mathbf{y}(x))\)</span>.</p>
<p>Now, the derivative of the RHS of <a href="#eq-firstderivative" class="quarto-xref">Eq.&nbsp;<span>10.67</span></a> is the product of the <span class="math inline">\(m\times(m+1)\)</span> matrix <a href="#eq-outer" class="quarto-xref">Eq.&nbsp;<span>10.68</span></a> and the <span class="math inline">\(m+1\)</span> dimensional vector <a href="#eq-inner" class="quarto-xref">Eq.&nbsp;<span>10.69</span></a>. This product has the general form <span class="math display">\[
\cdot
\begin{bmatrix}
C \\
D
\end{bmatrix}=A\cdot C+B\cdot D
\]</span> (which works for any block matrices, provided all the sizes are compatible, i.e.&nbsp;provided <span class="math inline">\(A\cdot C+B\cdot D\)</span> makes sense). We need to substitute <span class="math display">\[
A=\frac{\partial\mathbf{f}(x,\mathbf{y})}{\partial x}; \qquad
B= \frac{\partial\mathbf{f}(x,\mathbf{y})}{\partial \mathbf{y}}; \qquad
C= 1; \qquad
D= \mathbf{f}(x,\mathbf{y}(x))
\]</span> which gives us</p>
<p><span id="eq-ftotalcompute"><span class="math display">\[
= \underbrace{ \frac{\partial \mathbf{f}}{\partial x}(x,\mathbf{y}(x)) + \frac{\partial \mathbf{f}}{\partial \mathbf{y}}\cdot \mathbf{f}(x,\mathbf{y}(x)) }_{=: \frac{ \mathrm{d}^{} \mathbf{f} }{\mathrm{d}x^{} } (x,\mathbf{y}(x))}.
\tag{10.70}\]</span></span></p>
<p>We call the right-hand side the <em>total derivative</em> of <span class="math inline">\(\mathbf{f}\)</span>.</p>
<p>For higher derivatives, we can apply an analogous argument: We define recursively, <span class="math display">\[
\frac{ \mathrm{d}^{0} \mathbf{f}(x,\mathbf{y}) }{\mathrm{d}x^{0} }  := \mathbf{f}(x,\mathbf{y}), \quad
    \frac{ \mathrm{d}^{j+1} \mathbf{f}(x,\mathbf{y}) }{\mathrm{d}x^{j+1} }  :=
\frac{\partial}{\partial x}  \frac{ \mathrm{d}^{j} \mathbf{f}(x,\mathbf{y}) }{\mathrm{d}x^{j} }
+ \Big( \frac{\partial}{\partial \mathbf{y}}  \frac{ \mathrm{d}^{j} \mathbf{f}(x,\mathbf{y}) }{\mathrm{d}x^{j} }  \Big) \cdot \mathbf{f}(x,\mathbf{y}).
\]</span> Then we know that, when evaluated on the exact solution, we have</p>
<p><span id="eq-totaldery"><span class="math display">\[
    \frac{ \mathrm{d}^{j} \mathbf{f} }{\mathrm{d}x^{j} }  (x,\mathbf{y}(x)) =  \frac{ \mathrm{d}^{j+1} \mathbf{y} }{\mathrm{d}x^{j+1} } (x,\mathbf{y}).
\tag{10.71}\]</span></span></p>
<p>Hence, <a href="#eq-ytaylorn" class="quarto-xref">Eq.&nbsp;<span>10.65</span></a> rewrites to <span id="eq-ftaylorn"><span class="math display">\[
+ \mathbf{R}_i.
\tag{10.72}\]</span></span> This motivates us to define the <em>Taylor method of order <span class="math inline">\(n\)</span></em> by the difference equation <span class="math display">\[
\begin{aligned}
  \mathbf{w}_0 &amp;:= \boldsymbol{\alpha},
\\
  \mathbf{w}_{i+1} &amp;:= \mathbf{w}_{i} + h \mathbf{f}(x_i,\mathbf{w}_i)
+ \frac{h^2}{2}  \frac{ \mathrm{d}^{} \mathbf{f} }{\mathrm{d}x^{} } (x_i,\mathbf{w}_i)
+ \ldots
+ \frac{h^n}{n!}  \frac{ \mathrm{d}^{n-1} \mathbf{f} }{\mathrm{d}x^{n-1} }  (x_i,\mathbf{w}_i)
  \\
&amp;\hphantom{:}= \mathbf{w}_{i} + h \mathbf{T}_n(x_i,\mathbf{w}_i,h) ,
\end{aligned}
\]</span> where <span id="eq-tndef"><span class="math display">\[
\mathbf{T}_n(x,\mathbf{y},h) := \sum_{j=0}^{n-1} \frac{h^{j}}{(j+1)!}  \frac{ \mathrm{d}^{j} \mathbf{f} }{\mathrm{d}x^{j} } (x,\mathbf{y}).
\tag{10.73}\]</span></span></p>
<p>We have thus defined a one-step difference method with function <span class="math inline">\(\boldsymbol{\phi}=\mathbf{T}_n\)</span>. According to <a href="#thm-globalerr" class="quarto-xref">Theorem&nbsp;<span>10.3</span></a>, its global error is determined by its local truncation error, and by a Lipschitz constant for <span class="math inline">\(\mathbf{T}_n\)</span> (which we do not consider in detail here).</p>
<div id="thm-taylorerr" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.4</strong></span> The local truncation error of the Taylor method of order <span class="math inline">\(n\)</span> satisfies the bound <span class="math display">\[
\|\boldsymbol{\tau}_{i}(h)\| \leq \frac{h^{n}}{(n+1)!} \sup_{x \in [a,b]}
\Big\lVert   \frac{ \mathrm{d}^{n+1} \mathbf{y} }{\mathrm{d}x^{n+1} }  (x)  \Big\rVert=:\tau(h),
\]</span> supposing that the exact solution <span class="math inline">\(\mathbf{y}(x)\)</span> has <span class="math inline">\((n+1)\)</span> continuous derivatives.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By definition of the local truncation error in Eq.&nbsp;<a href="#eq-lteonestep" class="quarto-xref">Eq.&nbsp;<span>10.63</span></a>, we have <span class="math display">\[
\begin{aligned}
\|\boldsymbol{\tau}_{i+1}(h)\| &amp;= \frac{1}{h} \big\lVert  \mathbf{y}(x_{i+1}) - \mathbf{y}(x_{i}) - h \,\mathbf{T}_n(x_i,\mathbf{y}(x_i),h)  \big\rVert
\\
&amp;\overset{\eqref{eq:tndef}}{=} \frac{1}{h} \big\lVert  \mathbf{y}(x_{i+1}) - \mathbf{y}(x_{i})
- \sum_{j=0}^{n-1} \frac{h^{j+1}}{(j+1)!}  \frac{ \mathrm{d}^{j} \mathbf{f} }{\mathrm{d}x^{j} } (x_i,\mathbf{y}(x_i))  \big\rVert.
\end{aligned}
\]</span> However, comparing with <a href="#eq-ftaylorn" class="quarto-xref">Eq.&nbsp;<span>10.72</span></a>, this means <span class="math display">\[
\|\boldsymbol{\tau}_{i+1}(h)\| = \frac{1}{h} \lVert  \mathbf{R}_i  \rVert,
\]</span> and the proposed statement now follows from <a href="#eq-tmremainder" class="quarto-xref">Eq.&nbsp;<span>10.66</span></a>.&nbsp;◻</p>
</div>
<p>So, for any <span class="math inline">\(n\)</span>, we get an approximation method of order <span class="math inline">\(O(h^n)\)</span>. The Taylor method of order 1 coincides with Euler’s method. Thus, as a side result, we have proved that Euler’s method converges in the case of ODE systems.</p>
<div id="exm-eulerivpexample" class="example theorem" title="Euler's method for an IVP">
<p><span class="theorem-title"><strong>Example 10.7</strong></span> Let us once again consider the example IVP <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> from <a href="#eq-eulerivpexample" class="quarto-xref">Eq.&nbsp;<span>10.12</span></a>, <span class="math display">\[
y'(x) = y(x)-x^2+1,  \quad 0 \leq x \leq 1, \quad y(0) = \frac{1}{2}.
\]</span> Thus <span class="math inline">\(f(x,y)=y-x^2+1\)</span>. To set up the Taylor methods, say of order up to&nbsp;4, we need to compute the first 3 total derivatives of <span class="math inline">\(f\)</span>. In our example, this yields: <span class="math display">\[
\begin{aligned}
   \frac{ \mathrm{d}^{} f(x,y) }{\mathrm{d}x^{} }  &amp;= \frac{\partial f(x,y)}{\partial x}  +  \frac{\partial f(x,y)}{\partial y} f(x,y) = (-2x) +  1\cdot (y-x^2+1) = y-x^2-2x+1;
\\
   \frac{ \mathrm{d}^{2} f(x,y) }{\mathrm{d}x^{2} } &amp; = \frac{\partial}{\partial x} \frac{ \mathrm{d}^{} f(x,y) }{\mathrm{d}x^{} }   + \frac{\partial}{\partial y} \frac{ \mathrm{d}^{} f(x,y) }{\mathrm{d}x^{} }   f (x,y)
  = (-2x-2) + 1 \cdot (y-x^2+1) = y-x^2-2x-1;
\\
   \frac{ \mathrm{d}^{3} f(x,y) }{\mathrm{d}x^{3} } &amp; = \frac{\partial}{\partial x} \frac{ \mathrm{d}^{2} f(x,y) }{\mathrm{d}x^{2} }   + \frac{\partial}{\partial y} \frac{ \mathrm{d}^{2} (x,y)f }{\mathrm{d}x^{2} }   f(x,y)
  = (-2x-2) + 1 \cdot (y-x^2+1) = y-x^2-2x-1.
\end{aligned}
\]</span> In this simple example, we can now see that <em>all</em> higher-order total derivatives are equal to <span class="math inline">\(\mathrm{d}^2f(x,y)/\mathrm{d}x^2\)</span>; this will not normally be true.</p>
<p>Our second-order Taylor function <span class="math inline">\(T_2\)</span> is then <span class="math display">\[
T_2(x,y,h) = f(x,y) + \frac{h}{2}  \frac{ \mathrm{d}^{} f(x,y) }{\mathrm{d}x^{} }  = (y-x^2+1) + \frac{h}{2}(y-x^2-2x+1)
  = (1+\frac{h}{2}) (y-x^2+1)-hx.
\]</span> The second-order Taylor method therefore reads, <span class="math display">\[
w_0 := \frac{1}{2}, \quad
w_{i+1} := w_i+hT_2(x_i,w_i,h) = w_i + (h+\frac{h^2}{2})(w_i-x_i^2+1) - h^2 x_i.
\]</span> Analogously, one finds after a bit of computation that <span class="math display">\[
\begin{aligned}
  T_4(x,y,h) &amp;= f(x,y) + \frac{h}{2}  \frac{ \mathrm{d}^{} f(x,y) }{\mathrm{d}x^{} }
+ \frac{h^2}{6}  \frac{ \mathrm{d}^{2} f(x,y) }{\mathrm{d}x^{2} }  + \frac{h^3}{24}  \frac{ \mathrm{d}^{3} f(x,y) }{\mathrm{d}x^{3} }
\\&amp;= \Big( 1 +\frac{h}{2} + \frac{h^2}{6} + \frac{h^3}{24} \Big)(y-x^2)
+ \Big( 1 +\frac{h}{3} + \frac{h^2}{12} \Big) h x
+ \Big( 1 +\frac{h}{2} - \frac{h^2}{6} - \frac{h^3}{24} \Big).
\end{aligned}
\]</span></p>
</div>
<section id="advantages-and-disadvantages" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="advantages-and-disadvantages"><span class="header-section-number">10.5.1</span> Advantages and disadvantages</h3>
<p>We have seen that the error of the order <span class="math inline">\(n\)</span> Taylor method is <span class="math inline">\(O(h^n)\)</span>. This is much improved over the <span class="math inline">\(O(h^1)\)</span> of Euler’s method. Thus, in most situations, Taylor methods will give a much more accurate result.</p>
<p>However, the error estimate depends on the supremum of the <span class="math inline">\((n+1)\)</span>-th derivative of the exact solution, and on the Lipschitz constant for <span class="math inline">\(T_n\)</span>. We do not know a priori that these are small. Usually, this does not pose a problem in practice; we will however see some counterexamples in <a href="#sec-stiff" class="quarto-xref"><span>Section 10.9</span></a>.</p>
<p>Taylor methods can be constructed for any order <span class="math inline">\(n\)</span>, in a straightforward and unique way, by computing derivatives of the function <span class="math inline">\(f\)</span>.</p>
<p>Still, Taylor methods are not so frequently used in practice. The main reason for this is that they require us to compute the total derivatives of the function <span class="math inline">\(f\)</span> explicitly. In practice, this is too cumbersome to be done by hand. It can be achieved with symbolic differentiation algorithms, using computer algebra packages such as Maple. However, one would like to avoid this extra complexity. Also, it may not always be feasible to compute the derivatives explicitly: Suppose that, in a computer program, the function <span class="math inline">\(f\)</span> is given as a “black box” procedure that computes (or rather approximates) the function values <span class="math inline">\(f(x,y)\)</span> numerically; how would we gain access to the derivatives of <span class="math inline">\(f\)</span>?</p>
</section>
</section>
<section id="sec-rk" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="sec-rk"><span class="header-section-number">10.6</span> Runge-Kutta Methods</h2>
<p>Runge-Kutta methods are another example of one-step difference methods; they are very relevant in practice. They arise as modifications of the Taylor methods discussed in the previous section. Their main advantage is that they do <em>not</em> require us to compute derivatives of the function <span class="math inline">\(f\)</span> explicitly.</p>
<section id="motivation-the-modified-euler-method" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="motivation-the-modified-euler-method"><span class="header-section-number">10.6.1</span> Motivation: The Modified Euler Method</h3>
<p>The idea of Runge-Kutta methods is to avoid computing the total derivatives <span class="math inline">\(d^nf/dx^n\)</span>. Rather, these are replaced with <em>finite difference quotients</em>.</p>
<p>Recall that, if <span class="math inline">\(g\)</span> is a twice differentiable function, then we can use Taylor’s Theorem to write <span class="math display">\[
g(x+h)=g(x)+hg'(x)+O(h^2)
\]</span> and rearrange to give <span class="math display">\[
g'(x) = \frac{g(x+h)-g(x)}{h} + O(h).
\]</span> The fraction on the r.h.s. is called a finite difference quotient. We also remark that, by the MVT (or Taylor’s Theorem) <span class="math display">\[
g(x+h)=g(x)+O(h)
\]</span> We can replace <span class="math inline">\(h\)</span> by <span class="math inline">\(O(h^k)\)</span> here to give <span id="eq-bigOcommute"><span class="math display">\[
g(x+O(h^k))=g(x)+O(O(h^k))=g(x)+O(h^k)
\tag{10.74}\]</span></span></p>
<p>We start with the Taylor method of order two, here for <span class="math inline">\(m=1\)</span>, given by the function <span id="eq-t2repeat"><span class="math display">\[
   T_2(x,y,h) = f(x,y) + \frac{h}{2}  \frac{ \mathrm{d}^{} f }{\mathrm{d}x^{} }  (x,y).
\tag{10.75}\]</span></span></p>
<p>We use a difference quotient to approximate the total derivative, evaluated on the exact solution <span class="math inline">\(y(x)\)</span>: <span class="math display">\[
\frac{ \mathrm{d}^{} f }{\mathrm{d}x^{} }  (x,y(x)) = \frac{f\big(x+h,y(x+h)\big)-f\big(x,y(x)\big)}{h} + O(h).
\]</span> Inside this expression, for the term <span class="math inline">\(y(x+h)\)</span>, we use a Taylor approximation: <span class="math display">\[
y(x+h) = y(x) + h y'(x) + O(h^2)
= y(x) + h f\big(x,y(x)\big) + O(h^2).
\]</span> Using <a href="#eq-bigOcommute" class="quarto-xref">Eq.&nbsp;<span>10.74</span></a> we obtain <span class="math display">\[
\frac{ \mathrm{d}^{} f }{\mathrm{d}x^{} }  (x,y(x)) = \frac{1}{h} \Big( f\big(x+h,y(x)+hf(x,y(x)) \big)-f(x,y(x)) \Big) + O(h).
\]</span> Inserting into <a href="#eq-t2repeat" class="quarto-xref">Eq.&nbsp;<span>10.75</span></a>, we have <span class="math display">\[
T_2(x,y(x),h) = \frac{1}{2} f(x,y(x))
+ \frac{1}{2}   f\big(x+h,y(x)+hf(x,y(x)) \big)
  + O(h^2).
\]</span> Thus, if instead of the function <span class="math inline">\(T_2\)</span>, we use the following function for our one-step method, <span class="math display">\[
\phi(x,y,h) = \frac{1}{2} f(x,y)
+ \frac{1}{2}   f\big(x+h,y+hf(x,y) \big) ,
\]</span> we will incur an additional local truncation error of order <span class="math inline">\(O(h^2)\)</span>. However, this is not “much worse” than the Taylor method, which already has an error of order <span class="math inline">\(O(h^2)\)</span>.</p>
<p>The one-step difference method corresponding to <span class="math inline">\(\phi\)</span> is called the <em>Modified Euler method</em>. Its difference equation, <span class="math inline">\(w_{i+1}:=w_i + h\phi(x_i,w_i,h)\)</span>, can be rewritten in the following cleaned-up form: <span class="math display">\[
\begin{aligned}
   w_0 &amp;:= \alpha,
\\
k_{i,1} &amp;:= hf(x_i,w_i),
\\
k_{i,2} &amp;:= hf(x_i+h,w_i+k_{i,1}),
\\
w_{i+1} &amp;:= w_i + \frac{1}{2} k_{i,1} + \frac{1}{2} k_{i,2}.
\end{aligned}
\]</span> This shows in particular that only <em>two</em> evaluations of <span class="math inline">\(f\)</span> are needed in each step of the method. That is important to know, since the evaluation of <span class="math inline">\(f\)</span> is usually the time-consuming part when the method is implemented on a computer.</p>
<p>We have seen now, roughly, that the Modified Euler method works, and is of order <span class="math inline">\(O(h^2)\)</span>. A more complete proof will follow below. Let us first have a look at other methods of the same kind.</p>
</section>
<section id="general-runge-kutta-methods" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="general-runge-kutta-methods"><span class="header-section-number">10.6.2</span> General Runge-Kutta methods</h3>
<p>A general Runge-Kutta method is defined by a <em>Butcher tableau</em>:</p>
<p><span class="math display">\[
%
\begin{array}{c|ccccc}
   a_1 &amp;  &amp; &amp; &amp; &amp;
  \\
   a_2 &amp; b_{21} &amp; &amp; &amp; &amp;
  \\
   a_3 &amp; b_{31} &amp; b_{32} &amp; &amp; &amp;
  \\
   \vdots &amp; \vdots &amp; &amp; \ddots &amp; &amp;
  \\
   a_s &amp; b_{s1} &amp; b_{s2} &amp; \ldots &amp; b_{s,s-1} &amp;
\\
\cline{1-6}
    &amp; c_1 &amp; c_2 &amp; \ldots &amp; c_{s-1} &amp; c_s
%
\end{array}
\text{where $a_i$, $b_{ij}$, $c_i$ are real numbers.}
\]</span> The tableau is a shorthand notation for the associated difference equation of a one-step method: <span class="math display">\[
%
\begin{aligned}
\mathbf{w}_0 &amp;:= \alpha;
\\
\quad&amp;\quad      \mathbf{k}_1 := h \mathbf{f}(x_i + a_1 h, \mathbf{w}_i + 0),
  \\
\quad&amp;\quad      \mathbf{k}_2 := h \mathbf{f}(x_i + a_2 h, \mathbf{w}_i + b_{21} \mathbf{k}_1),
  \\
      \quad&amp;\quad \mathbf{k}_3 := h \mathbf{f}(x_i + a_3 h, \mathbf{w}_i + b_{31} \mathbf{k}_1 + b_{32} \mathbf{k}_2),
  \\
   \quad&amp;\quad  \qquad \vdots
  \\
   \quad&amp;\quad \mathbf{k}_s := h \mathbf{f}(x_i + a_s h, \mathbf{w}_i + \sum_{j=1}^{s-1} b_{sj} \mathbf{k}_j);
\\
    \mathbf{w}_{i+1} &amp;:= \mathbf{w}_i + \sum_{j=1}^s c_j \mathbf{k}_j
%
\end{aligned}
%
\]</span> (The <span class="math inline">\(\mathbf{k}_j\)</span> depend in addition on the step <span class="math inline">\(i\)</span>, but they are regarded as “intermediate results”, and we do not denote this dependence explicitly.) Our Modified Euler method is an example of such a scheme, namely with the tableau <span class="math display">\[
%
\begin{array}{c|cc}
   0
  \\
   1 &amp; 1
\\
  \hline
    &amp; \;\tfrac{1}{2} \; &amp; \;\tfrac{1}{2}  \;
%
\end{array}
\]</span></p>
<p>We do not discuss here in general how these tableaux are obtained, or how to prove in general of what order their local truncation error is. However, here are some more examples.</p>
<p>The <em>Midpoint method</em> is another method of order <span class="math inline">\(O(h^2)\)</span>. It is given by the tableau <span class="math display">\[
%
\begin{array}{c|cc}
   0
  \\
   \tfrac{1}{2} &amp; \tfrac{1}{2}
\\
  \hline
    &amp; \;0 \; &amp; \;1 \;
%
\end{array}
\]</span> and its difference equation can be written explicitly as <span class="math display">\[
\mathbf{w}_0 := \boldsymbol{\alpha}, \quad \mathbf{w}_{i+1} = \mathbf{w}_i + h \, \mathbf{f}\big(x_i + \frac{h}{2}, \mathbf{w}_i + \frac{h}{2} \mathbf{f}(x_i,\mathbf{w}_i)
\big).
\]</span> There are more methods of order <span class="math inline">\(O(h^2)\)</span>; for example, <em>Heun’s method</em>: <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="math display">\[
\setlength\extrarowheight{5pt}
%
\begin{array}{c|cc}
   0
  \\
   \tfrac{2}{3} &amp; \tfrac{2}{3}
\\
  \hline
    &amp; \;\tfrac{1}{4} \; &amp; \;\tfrac{3}{4}  \;
%
\end{array}
\]</span> The explicit form of the difference equation is <span class="math display">\[
\mathbf{w}_0 := \boldsymbol{\alpha}, \quad \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{4} \Big(\mathbf{f}(x_i,\mathbf{w}_i) + 3 \mathbf{f}\big(x_i
+ \frac{2}{3}h, \mathbf{w}_i + \frac{2}{3}{h} \,\mathbf{f}(x_i,\mathbf{w}_i) \big) \Big).
\]</span> Finally, let us mention the “classical” Runge-Kutta method <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> of order <span class="math inline">\(O(h^4)\)</span>. This method is widely used in practice. Its Butcher tableau is</p>
<p><span id="eq-rk4classical"><span class="math display">\[
\end{array}
\tag{10.76}\]</span></span> and it is <em>not</em> very useful to write its difference equation in one line! (In general, when used in programming, working with the intermediate results <span class="math inline">\(\mathbf{k}_i\)</span> is a good way of organizing the code.)</p>
</section>
<section id="local-truncation-error-for-second-order-methods" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="local-truncation-error-for-second-order-methods"><span class="header-section-number">10.6.3</span> Local truncation error for second-order methods</h3>
<p>The general idea of proving a rigorous error estimate for a Runge-Kutta method is comparing it to the Taylor method of order <span class="math inline">\(n\)</span>, with <span class="math inline">\(n\)</span> appropriately chosen. Given the function <span class="math inline">\(\boldsymbol{\phi}_\text{RK}(x,y,h)\)</span> that defines the method (as a one-step difference method), one would try to prove that</p>
<p><span id="eq-rktaylorcompare"><span class="math display">\[
   \leq c h^n
\tag{10.77}\]</span></span> with a constant <span class="math inline">\(c&gt;0\)</span>. This proof will usually involve a Taylor expansion of the function <span class="math inline">\(\mathbf{f}\)</span> within <span class="math inline">\(\boldsymbol{\phi}_\text{RK}\)</span>, and the constant <span class="math inline">\(c\)</span> will depend on estimates for the function <span class="math inline">\(\mathbf{f}\)</span> and its derivatives. Once <a href="#eq-rktaylorcompare" class="quarto-xref">Eq.&nbsp;<span>10.77</span></a> is known, it follows from the definition of the local truncation error <a href="#eq-lteonestep" class="quarto-xref">Eq.&nbsp;<span>10.63</span></a> and the triangle inequality that <span class="math display">\[
\tau_\text{RK}(h) \leq \tau_\text{Taylor-$n$}(h) + c \,h^n.
\]</span> With the estimate for <span class="math inline">\(\tau_\text{Taylor-$n$}(h)\)</span> (the truncation error of the <span class="math inline">\(n\)</span>-th order Taylor method) known from <a href="#thm-taylorerr" class="quarto-xref">Theorem&nbsp;<span>10.4</span></a> this yields <span class="math display">\[
\tau_\text{RK}(h) \leq c' \,h^n.
\]</span> with another constant <span class="math inline">\(c'&gt;0\)</span>. <a href="#thm-globalerr" class="quarto-xref">Theorem&nbsp;<span>10.3</span></a> then tells us that the global error of the Runge-Kutta method is of order <span class="math inline">\(O(h^n)\)</span>.</p>
<p>However, for most higher-order methods, such as the classical Runge-Kutta method <a href="#eq-rk4classical" class="quarto-xref">Eq.&nbsp;<span>10.76</span></a> of order <span class="math inline">\(O(h^4)\)</span>, the proof of <a href="#eq-rktaylorcompare" class="quarto-xref">Eq.&nbsp;<span>10.77</span></a> is rather tedious, since the computation becomes quite lengthy.</p>
<p>Here, we will give the proof only for <span class="math inline">\(O(h^2)\)</span> methods with a Butcher tableau of the form <span id="eq-butcherorder2"><span class="math display">\[
\quad \text{where } c,d \in (0,1], \quad cd = \frac{1}{2}.
\tag{10.78}\]</span></span> This includes the Modified Euler method (<span class="math inline">\(c=1\)</span>, <span class="math inline">\(d=1/2\)</span>), the Midpoint method (<span class="math inline">\(c=1/2\)</span>, <span class="math inline">\(d=1\)</span>), and Heun’s method (<span class="math inline">\(c=2/3\)</span>, <span class="math inline">\(d=3/4\)</span>).</p>
<div id="thm-rkerror" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10.5</strong></span> Consider a Runge-Kutta method of the form <a href="#eq-butcherorder2" class="quarto-xref">Eq.&nbsp;<span>10.78</span></a>. Suppose that the function <span class="math inline">\(\mathbf{f}\)</span> and all its derivatives up to order <span class="math inline">\(2\)</span> are bounded. <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Then, there exists a constant <span class="math inline">\(C\)</span> such that the local truncation error is bounded by <span class="math display">\[
\|\boldsymbol{\tau}_i(h)\| \leq C h^2.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>As explained above, we need to prove <a href="#eq-rktaylorcompare" class="quarto-xref">Eq.&nbsp;<span>10.77</span></a> in our case. The function <span class="math inline">\(\boldsymbol{\phi}\)</span> is here given by <span id="eq-phirk2"><span class="math display">\[
  \boldsymbol{\phi}(x,\mathbf{y},h) = (1-d)\, \mathbf{f}(x,\mathbf{y}) + d \,\underbrace{\mathbf{f}\big(x+ch, \mathbf{y}+ch\mathbf{f}(x,\mathbf{y})\big)}_{=:\mathbf{k}},
\tag{10.79}\]</span></span></p>
<p>and we need to compare it to the Taylor method of order 2, given by <span class="math display">\[
\mathbf{T}_2(x,y,h) =  \mathbf{f}(x,\mathbf{y}) + \frac{h}{2}  \frac{ \mathrm{d}^{} \mathbf{f} }{\mathrm{d}x^{} } (x, \mathbf{y}).
\]</span> We first take the term <span class="math inline">\(\mathbf{k}\)</span> from <a href="#eq-phirk2" class="quarto-xref">Eq.&nbsp;<span>10.79</span></a>, and use a first-order Taylor expansion of the function <span class="math inline">\(\mathbf{f}\)</span> around the point <span class="math inline">\((x,\mathbf{y})\)</span>, in the form of <span class="quarto-unresolved-ref">?thm-taylororder1</span>. This gives <span id="eq-kcompute"><span class="math display">\[
\end{aligned}
\tag{10.80}\]</span></span> We assume that all second partial derivatives of <span class="math inline">\(\mathbf{f}\)</span> (by <span class="math inline">\(x\)</span> or <span class="math inline">\(y_j\)</span>) are bounded by a constant <span class="math inline">\(M\)</span>. According to <span class="quarto-unresolved-ref">?thm-taylororder1</span>, the Taylor remainder term <span class="math inline">\(\mathbf{R}\)</span> is then bounded by <span class="math display">\[
\lVert \mathbf{R} \rVert \leq \frac{1}{2} (ch)^2 \lVert  (1, f_1, \ldots, f_m)  \rVert^2
  \cdot (m+1)^2 M.
\]</span> After possibly modifying the constant <span class="math inline">\(M\)</span>, we can assume that <span class="math inline">\(\|f_j\|\leq M\)</span> as well, and that <span class="math inline">\(M \geq 1\)</span>. This gives <span class="math display">\[
\lVert \mathbf{R} \rVert \leq \frac{(m+1)^2 M^3 c^2}{2} h^2.
\]</span> Inserting <a href="#eq-kcompute" class="quarto-xref">Eq.&nbsp;<span>10.80</span></a> into <a href="#eq-phirk2" class="quarto-xref">Eq.&nbsp;<span>10.79</span></a> yields <span class="math display">\[
\boldsymbol{\phi}(x,\mathbf{y},h) = (1-d) \mathbf{f}(x,\mathbf{y}) + d \mathbf{k}
= \mathbf{f}(x,\mathbf{y}) + cdh   \frac{ \mathrm{d}^{} \mathbf{f} }{\mathrm{d}x^{} } (x,\mathbf{y}) + d\mathbf{R}= \mathbf{T}_2(x,y,h)+d\mathbf{R},
\]</span> since <span class="math inline">\(cd=1/2\)</span>. Consequently, <span class="math display">\[
\lVert  \boldsymbol{\phi}(x,\mathbf{y},h) - \mathbf{T}_2(x,\mathbf{y},h) \rVert \leq \frac{(m+1)^2 M^3 c}{4} h^2,
\]</span> which completes the proof.&nbsp;◻</p>
</div>
</section>
<section id="advantages-and-disadvantages-1" class="level3" data-number="10.6.4">
<h3 data-number="10.6.4" class="anchored" data-anchor-id="advantages-and-disadvantages-1"><span class="header-section-number">10.6.4</span> Advantages and disadvantages</h3>
<p>Runge-Kutta methods are very widely used in practice. While they are slightly less accurate than Taylor methods, they can be chosen of the same order <span class="math inline">\(O(h^n)\)</span>, and in this sense they are “as good” as Taylor methods. However, they have the advantage that it is not necessary to compute the total derivatives of <span class="math inline">\(\mathbf{f}\)</span> explicitly. They are rather straightforward to implement on a computer, given the Butcher tableau of a method. Particularly, the “classical” Runge-Kutta method is a good choice where one needs an algorithm of reasonable accuracy which is easy to implement.</p>
<p>However, the error bounds of Runge-Kutta methods are sensitive to the higher-order derivatives of <span class="math inline">\(\mathbf{f}\)</span>. (They share this problem with Taylor methods.) In some cases, to be discussed in <a href="#sec-stiff" class="quarto-xref"><span>Section 10.9</span></a>, this leads to problems.</p>
</section>
</section>
<section id="sec-rkf" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="sec-rkf"><span class="header-section-number">10.7</span> Error Control, Runge-Kutta-Fehlberg Method</h2>
<section id="error-control" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="error-control"><span class="header-section-number">10.7.1</span> Error control</h3>
<p>We have so far discussed how the approximation error changes with the step size <span class="math inline">\(h\)</span>, but we have not really explained how to choose <span class="math inline">\(h\)</span> in practice.</p>
<p>In principle, given an IVP, one might first try to prove rigorous error estimates, like in the example in <a href="#sec-euler" class="quarto-xref"><span>Section 10.2</span></a>, but with <span class="math inline">\(h\)</span> left open. Then one can choose <span class="math inline">\(h\)</span> according to the maximum error one wants to allow. However, these estimates are very hard (if not impossible) to compute in realistic examples.</p>
<p>Instead, one would like to have an approximation algorithm that automatically computes an estimate for the error, at least roughly, and that chooses the step size <span class="math inline">\(h\)</span> accordingly. This technique is known as <em>error control</em>.</p>
<p>The idea is roughly as follows. Let us use <em>two</em> approximation methods, with approximation values <span class="math inline">\(\mathbf{w}_i\)</span> and <span class="math inline">\(\tilde \mathbf{w}_i\)</span>. Suppose that <span class="math inline">\(\tilde \mathbf{w}_i\)</span> is much more exact than <span class="math inline">\(\mathbf{w}_i\)</span>, i.e., that the absolute error of <span class="math inline">\(\tilde \mathbf{w}_i\)</span> is much smaller than that of <span class="math inline">\(\mathbf{w}_i\)</span>. (For example, the second approximation method might be of higher order in <span class="math inline">\(h\)</span>.) Then we have <span class="math display">\[
\lVert  \mathbf{y}(x_i)-\mathbf{w}_i  \rVert \leq \underbrace{ \lVert  \mathbf{y}(x_i)-\tilde \mathbf{w}_i  \rVert }_{\text{negligible}}
+ \lVert \tilde \mathbf{w}_i-\mathbf{w}_i  \rVert \approx \lVert \tilde \mathbf{w}_i-\mathbf{w}_i \rVert.
\]</span> However, the right-hand side can be computed without knowing the exact solution <span class="math inline">\(\mathbf{y}(x)\)</span>.</p>
<p>To make this more concrete, let us say that <span class="math inline">\(\mathbf{w}_i\)</span> and <span class="math inline">\(\tilde \mathbf{w}_i\)</span> are computed by two one-step difference methods, with defining functions <span class="math inline">\(\boldsymbol{\phi}\)</span> and <span class="math inline">\(\tilde \boldsymbol{\phi}\)</span>, respectively. Let us assume that <span class="math inline">\(\boldsymbol{\phi}\)</span> yields a local truncation error <span class="math inline">\(\|\boldsymbol{\tau}_{i}(h)\|\)</span> of order <span class="math inline">\(O(h^n)\)</span>, while the local truncation error <span class="math inline">\(\|\tilde \boldsymbol{\tau}_{i}(h)\|\)</span> of <span class="math inline">\(\tilde \boldsymbol{\phi}\)</span> is of order <span class="math inline">\(O(h^{n+1})\)</span>.</p>
<p>Further, we fix <span class="math inline">\(i\)</span> and suppose that <span class="math inline">\(\mathbf{y}(x_i)=\mathbf{w}_i=\tilde \mathbf{w}_i\)</span>, that is, we start form an exact value in the previous step. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> For given step size <span class="math inline">\(h\)</span>, we want to estimate the local truncation error <span class="math inline">\(\|\boldsymbol{\tau}_{i+1}(h)\|\)</span> of the first method, supposing that <span class="math inline">\(\tilde\boldsymbol{\tau}_{i+1}\)</span> is negligible compared with it. <span id="eq-lterkf"><span class="math display">\[
\begin{gathered}
   \|\boldsymbol{\tau}_{i+1}(h)\| = \frac{1}{h} \lVert \mathbf{y}(x_{i+1})-\mathbf{y}(x_i) -h \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)  \rVert
\\
= \frac{1}{h} \lVert  \underbrace{\big(\mathbf{y}(x_{i+1})-\mathbf{y}(x_i) -h \tilde \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)\big)}_{\text{$\sim h\|\tilde\boldsymbol{\tau}_{i+1}\|$, negligible} }
  + h \tilde \boldsymbol{\phi}(x_i,\mathbf{w}_i,h) -h \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)  \rVert
\\
\approx
  \lVert  \tilde \boldsymbol{\phi}(x_i,\mathbf{w}_i,h) - \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)  \rVert.
\end{gathered}
\tag{10.81}\]</span></span> Thus we can determine the local truncation error approximately by evaluating the functions <span class="math inline">\(\boldsymbol{\phi}\)</span> and <span class="math inline">\(\tilde\boldsymbol{\phi}\)</span>.</p>
<p><span class="math display">\[
%
\begin{array}{c|ccccccl}
   0 &amp;  
  \\
   \frac{1}{4} &amp; \frac{1}{4}
  \\
   \frac{3}{8} &amp; \frac{3}{32} &amp; \frac{9}{32}
  \\
   \frac{12}{13} &amp; \frac{1932}{2197} &amp; -\frac{7200}{2197} &amp; \frac{7296}{2197}
  \\
   1 &amp; \frac{439}{216} &amp; -8 &amp; \frac{3680}{513} &amp; - \frac{845}{4104}
\\
   \frac{1}{2} &amp; -\frac{8}{27} &amp; 2 &amp; -\frac{3544}{2565} &amp;  \frac{1859}{4104} &amp; -\frac{11}{40}
%
\\
\hline
  \text{\textbf{(a)}}  &amp; \frac{25}{216} &amp; 0 &amp; \frac{1408}{2565} &amp;  \frac{2197}{4104} &amp; -\frac{1}{5} &amp;
%
\\
\hline
  \text{\textbf{(b)}}  &amp; \frac{16}{135} &amp; 0 &amp; \frac{6656}{12825} &amp; \frac{28561}{56430} &amp; - \frac{9}{50} &amp; \frac{2}{55}
%
\end{array}
%
\]</span></p>
<p>However, what we actually want is to choose the step size <span class="math inline">\(h\)</span> so that <span class="math inline">\(\|\boldsymbol{\tau}_{i+1}(h)\| \leq T\)</span> with some given “tolerance” value <span class="math inline">\(T\)</span>. To that end, let us assume that <span class="math inline">\(\|\boldsymbol{\tau}_{i+1}(h)\|\)</span> is not only of order <span class="math inline">\(O(h^n)\)</span>, but in fact (roughly) proportional to <span class="math inline">\(h^n\)</span>: <span class="math display">\[
   \|\boldsymbol{\tau}_{i+1} (h)\| \approx K \, h^n \quad \text{for some $K&gt;0$}.
\]</span> We proceed as follows. We start with some fixed step size <span class="math inline">\(h\)</span> and compute the related function values: <span id="eq-tauh"><span class="math display">\[
   \|\boldsymbol{\tau}_{i+1} (h)\| \approx K \, h^n  \approx \lVert  \tilde \boldsymbol{\phi}(x_i,\mathbf{w}_i,h) - \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)  \rVert.
\tag{10.82}\]</span></span></p>
<p>Now we want to adjust <span class="math inline">\(h\)</span> by a factor <span class="math inline">\(q&gt;0\)</span>, such that <span class="math inline">\(\|\boldsymbol{\tau}_{i+1}(qh)\| \approx T\)</span>. By our assumption <span class="quarto-unresolved-ref">?eq-ltescale</span>, <span id="eq-tauqh"><span class="math display">\[
   \|\boldsymbol{\tau}_{i+1} (qh)\| \approx K \, (qh)^n  \approx T.
\tag{10.83}\]</span></span></p>
<p>Dividing <a href="#eq-tauqh" class="quarto-xref">Eq.&nbsp;<span>10.83</span></a> by <a href="#eq-tauh" class="quarto-xref">Eq.&nbsp;<span>10.82</span></a> yields <span class="math display">\[
q^n\approx \frac{T}{\lVert  \tilde \boldsymbol{\phi}(x_i,\mathbf{w}_i,h) - \boldsymbol{\phi}(x_i,\mathbf{w}_i,h)  \rVert},
\]</span> thus we should choose <span id="eq-qtheory"><span class="math display">\[
   =  \left( \frac{hT}{\lVert  \tilde \mathbf{w}_{i+1} - \mathbf{w}_{i+1}  \rVert }  \right)^{1/n},
\tag{10.84}\]</span></span></p>
<p>where it is understood that <span class="math inline">\(\mathbf{w}_{i+1},\tilde\mathbf{w}_{i+1}\)</span> are computed with <span class="math inline">\(\boldsymbol{\phi},\tilde \boldsymbol{\phi}\)</span> using the step size <span class="math inline">\(h\)</span>.</p>
</section>
<section id="the-runge-kutta-fehlberg-method" class="level3" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="the-runge-kutta-fehlberg-method"><span class="header-section-number">10.7.2</span> The Runge-Kutta-Fehlberg method</h3>
<p>A popular implementation of the above principle is the <em>Runge-Kutta-Fehlberg</em> method. It uses an order-4 Runge-Kutta method for <span class="math inline">\(\boldsymbol{\phi}\)</span> and an order-5 Runge-Kutta method for <span class="math inline">\(\tilde\boldsymbol{\phi}\)</span>. The Butcher tableaux for both methods are shown in <span class="quarto-unresolved-ref">?tbl-butcherrkf</span>. The special feature of this algorithm is that the two approximation methods <em>share</em> most parts of their coefficient schemes; more precisely, the auxiliary values <span class="math inline">\(\mathbf{k}_1,\ldots,\mathbf{k}_6\)</span> are the same in both approximation methods. Only the “final lines” in the Butcher tableaux are different: for computing <span class="math inline">\(\mathbf{w}_{i+1}\)</span>, one uses the line marked (a), while for <span class="math inline">\(\tilde \mathbf{w}_{i+1}\)</span>, the line marked (b) is used. This arrangement makes the algorithm rather efficient: the function <span class="math inline">\(\mathbf{f}\)</span> needs to be evaluated only 6 times per step. Compare this with the 4 function evaluations needed for the classical Runge-Kutta method of order 4 <em>without</em> error control. The additional overhead caused by the error control method is noticeable, but not too large.</p>
<div class="algorithm">
<div class="algorithmic">
<p><span class="math inline">\(x := a\)</span>, <span class="math inline">\(\mathbf{w}:= \boldsymbol{\alpha}\)</span>, <span class="math inline">\(h := h_{max}\)</span> Compute <span class="math inline">\(\mathbf{k}_1,\ldots, \mathbf{k}_6\)</span> from Butcher tableau with step size <span class="math inline">\(h\)</span> <span class="math inline">\(R := {h}^{-1}\lVert  \sum_{j=1}^6  (c_j-\tilde c_j) \mathbf{k}_j \rVert\)</span> <span class="math inline">\(x := x+h\)</span>, <span class="math inline">\(\mathbf{w}:= \mathbf{w}+ \sum_{j=1}^5  c_j \mathbf{k}_j\)</span> <span class="math inline">\(q :=(T/2R)^{1/4}\)</span> <span class="math inline">\(h := 0.1h\)</span> <span class="math inline">\(h := 4h\)</span> <span class="math inline">\(h := h \cdot q\)</span> <strong>if</strong> <span class="math inline">\(h \geq h_{max}\)</span> <strong>then</strong> <span class="math inline">\(h:= h_{max}\)</span> <strong>end if</strong> <span class="math inline">\(h := b-x\)</span> <strong>exception</strong> (“minimum step size exceeded”)</p>
<p><span class="math inline">\(\mathbf{w}\)</span></p>
</div>
</div>
<p>Algorithm&nbsp;<a href="#alg:rkf" data-reference-type="ref" data-reference="alg:rkf">[alg:rkf]</a> shows the Runge-Kutta-Fehlberg method in pseudocode. It works roughly by the principles discussed above, but with some modification in detail. In lines 4–8, the next approximation value <span class="math inline">\(\mathbf{w}_{i+1}\)</span> is computed; however, this happens only if the estimate for the local truncation error, computed according to <a href="#eq-lterkf" class="quarto-xref">Eq.&nbsp;<span>10.81</span></a>, is below the specified tolerance <span class="math inline">\(T\)</span>. Then, in lines 9–22, the step size <span class="math inline">\(h\)</span> for the next step is computed. In slight deviation from <a href="#eq-qtheory" class="quarto-xref">Eq.&nbsp;<span>10.84</span></a>, one chooses a more conservative factor, <span class="math display">\[
q =  \left( \frac{hT}{2 \lVert  \tilde \mathbf{w}_{i+1} - \mathbf{w}_{i+1}  \rVert }  \right)^{1/4} \quad \text{(note $n=4$)}.
\]</span> Also, the factor <span class="math inline">\(q\)</span> is limited to the interval <span class="math inline">\([0.1,4]\)</span> (lines 10–16), in order to avoid possible instabilities by rapid changes in the step size. Further, the step size is never increased beyond a given value <span class="math inline">\(h_\text{max}\)</span> (line 17); and if it decreases below a minimum step size <span class="math inline">\(h_\text{min}\)</span>, we terminate with an error message (line 21) – otherwise we might run into problems with limited floating point precision. Some extra handling is needed for the last step of the approximation, in order to ensure that we end up exactly at the right-hand boundary of the interval <span class="math inline">\([a,b]\)</span> (line 19).</p>
</section>
</section>
<section id="multi-step-methods" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="multi-step-methods"><span class="header-section-number">10.8</span> Multi-Step Methods</h2>
<p><em>N.B.: This section is intended as a brief overview, and its material is not examinable. More information can be found, e.g.,&nbsp;in <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010, sec. 5.6</a>)</span>.</em></p>
<p>Multi-step methods are an altervative way of approximating the solution of the initial value problem <span id="eq-multiivp"><span class="math display">\[
\mathbf{y}' = \mathbf{f}(x,\mathbf{y}),  \quad a \leq x \leq b, \quad \mathbf{y}(a) = \boldsymbol{\alpha}.
\tag{10.85}\]</span></span></p>
<p>Again, they are generalizations of Euler’s method, and we use the same division of <span class="math inline">\([a,b]\)</span> into equally spaced mesh points <span class="math inline">\(x_0,\ldots,x_n\)</span>. But rather than using several evaluations of the function <span class="math inline">\(\mathbf{f}\)</span> in each approximation step, like Runge-Kutta methods do, we make use of the values of <span class="math inline">\(\mathbf{f}\)</span> that have already been computed at previous mesh points.</p>
<p>A general <em><span class="math inline">\(k\)</span>-step multi-step method</em> is given by a difference equation</p>
<p><span id="eq-multistepdef"><span class="math display">\[
\end{aligned}
\tag{10.86}\]</span></span> Here <span class="math inline">\(a_0 \ldots a_{k-1}, b_0 \ldots b_{k} \in \mathbb{R}\)</span> are constants that define the method. <span class="math inline">\(\boldsymbol{\alpha}_1 \ldots \boldsymbol{\alpha}_{k-1} \in\mathbb{R}^m\)</span> are certain starting values that are needed for the difference equation to work; we will discuss later how to obtain them. If <span class="math inline">\(b_k=0\)</span>, the method is called <em>explicit</em>; otherwise, it is called <em>implicit</em>. In the latter case, <span class="math inline">\(\mathbf{w}_{i+1}\)</span> appears on both sides of the difference equation, and is hence defined only implicitly; we will discuss later how such methods can be used.</p>
<p>Like for one-step methods, one defines the <em>local truncation error</em> of the multistep method <a href="#eq-multistepdef" class="quarto-xref">Eq.&nbsp;<span>10.86</span></a> by <span id="eq-ltemulti"><span class="math display">\[
   \right].
\tag{10.87}\]</span></span> Again, the local truncation error determines the global error, i.e., an analogue of <a href="#thm-globalerr" class="quarto-xref">Theorem&nbsp;<span>10.3</span></a> for multi-step methods holds. However, we do not discuss this here.</p>
<p>Examples of <span class="math inline">\(k\)</span>-step multistep methods are the so-called Adams-Bashforth and Adams-Moulton methods. They are derived from rewriting the differential equation as an integral equation, <span class="math display">\[
\mathbf{y}'=\mathbf{f}(x,\mathbf{y}(x)) \quad \Leftrightarrow \quad
  \mathbf{y}(x_{i+1}) = \mathbf{y}(x_i) + \int_{x_i}^{x_{i+1}} \mathbf{f}(x,\mathbf{y}(x)) \,dx,
\]</span> and then approximating the components <span class="math inline">\(\hat{f}_j\)</span> on the right hand side with Lagrange interpolating polynomials. The integral can be done explicitly, and one obtains the difference equation of a multi-step method; see <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010, sec. 5.6</a>)</span> for details. The number <span class="math inline">\(k\)</span> of steps depends on the number of interpolation points chosen. Adams-Bashforth methods (<a href="#tbl-abmethod" class="quarto-xref">Table&nbsp;<span>10.2</span></a>) are <span class="math inline">\(k\)</span>-step <em>explicit</em> methods of error order <span class="math inline">\(O(h^k)\)</span> and Adams-Moulton methods (<a href="#tbl-ammethod" class="quarto-xref">Table&nbsp;<span>10.3</span></a>) are <span class="math inline">\(k\)</span>-step <em>implicit</em> methods of error oreder <span class="math inline">\(O(h^{k+1})\)</span>,</p>
<p>Notation: <span class="math inline">\(\mathbf{f}_j := \mathbf{f}(x_j,\mathbf{w}_j)\)</span>.</p>
<div id="tbl-abmethod" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-abmethod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.2: : Adams-Bashforth methods
</figcaption>
<div aria-describedby="tbl-abmethod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Difference equation</th>
<th style="text-align: center;">Error order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Two-step explicit</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(O(h^2)\)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">_{i+1} = _i + (3 <em>i - </em>{i-1})</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">%</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Three-step explicit<br>
<span class="math display">\[
%                                                                                                                                   $O(h^3)$
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{12} (23 \mathbf{f}_i - 16 \mathbf{f}_{i-1} + 5 \mathbf{f}_{i-2})                         
                        %
\]</span></p>
<p>Four-step explicit<br>
<span class="math display">\[
%                                                                                                                                   $O(h^4)$
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{24} (55 \mathbf{f}_i - 59 \mathbf{f}_{i-1} + 37 \mathbf{f}_{i-2} - 9 \mathbf{f}_{i-3})   
                        %
\]</span></p>
<p>Five-step explicit<br>
<span class="math display">\[
\begin{gathered}                                                                                                                    $O(h^5)$
                        %                                                                                                                                   
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{720} (1901 \mathbf{f}_i - 2774 \mathbf{f}_{i-1}                                          
                         \\ + 2616 \mathbf{f}_{i-2} - 1274 \mathbf{f}_{i-3} + 251 \mathbf{f}_{i-4})                                                         
                        %                                                                                                                                   
                        \end{gathered}
\]</span></p>
</div>
</figure>
</div>
<p>As with Taylor and Runge-Kutta methods, the estimates for the local truncation error depend on higher-order total derivatives of <span class="math inline">\(\mathbf{f}\)</span>.</p>
<p>In order to apply these multi-step methods in examples, we need additional start values <span class="math inline">\(\mathbf{w}_1=\boldsymbol{\alpha}_1, \ldots,
\mathbf{w}_{k-1} = \boldsymbol{\alpha}_{k-1}\)</span>. These are normally obtained by using one-step methods of the same error order. For example, for the four-step Adams-Bashforth method, one might use the classical Runge-Kutta method of order <span class="math inline">\(O(h^4)\)</span>.</p>
<p>Notation: <span class="math inline">\(\mathbf{f}_j := \mathbf{f}(x_j,\mathbf{w}_j)\)</span>.</p>
<div id="tbl-ammethod" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ammethod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.3: : Adams-Moulton methods
</figcaption>
<div aria-describedby="tbl-ammethod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Difference equation</th>
<th style="text-align: center;">Error order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Two-step implicit</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(O(h^3)\)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">_{i+1} = <em>i + (5 </em>{i+1} + 8 <em>i - </em>{i-1})</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">%</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Three-step implicit<br>
<span class="math display">\[
%                                                                                                                               $O(h^4)$
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{24} (9 \mathbf{f}_{i+1} + 19 \mathbf{f}_i - 5 \mathbf{f}_{i-1} + \mathbf{f}_{i-2})   
                        %
\]</span></p>
<p>Four-step implicit<br>
<span class="math display">\[
\begin{gathered}                                                                                                                $O(h^5)$
                        %                                                                                                                               
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{720} (251 \mathbf{f}_{i+1} + 646 \mathbf{f}_i - 264 \mathbf{f}_{i-1}                 
                        \\+ 106                                                                                                                         
                        \mathbf{f}_{i-2} - 19 \mathbf{f}_{i-3})                                                                                         
                        %                                                                                                                               
                        \end{gathered}
\]</span></p>
<p>Five-step implicit<br>
<span class="math display">\[
\begin{gathered}                                                                                                                $O(h^6)$
                        %                                                                                                                               
                        \mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{1440} (475 \mathbf{f}_{i+1} + 1427 \mathbf{f}_i - 798 \mathbf{f}_{i-1}               
                        \\+ 482                                                                                                                         
                        \mathbf{f}_{i-2} - 173 \mathbf{f}_{i-3} + 27 \mathbf{f}_{i-4})                                                                  
                        %                                                                                                                               
                        \end{gathered}
\]</span></p>
</div>
</figure>
</div>
<section id="predictor-corrector-methods" class="level3" data-number="10.8.1">
<h3 data-number="10.8.1" class="anchored" data-anchor-id="predictor-corrector-methods"><span class="header-section-number">10.8.1</span> Predictor-corrector methods</h3>
<p>Implicit multi-step methods, like the Adams-Moulton methods shown in <a href="#tbl-ammethod" class="quarto-xref">Table&nbsp;<span>10.3</span></a>, involve the term <span class="math inline">\(\mathbf{f}(x_{i+1},\mathbf{w}_{i+1})\)</span> on the right-hand side of the difference equation. This means that <span class="math inline">\(\mathbf{w}_{i+1}\)</span> is defined only implicitly; it is unclear at first how these methods should be used in practice.</p>
<p>One option would be to solve the difference equation for <span class="math inline">\(\mathbf{w}_{i+1}\)</span>, either symbolically (for simple cases of <span class="math inline">\(\mathbf{f}\)</span>) or numerically. This is useful only in very specific cases; we will come back to this approach in <a href="#sec-stiff" class="quarto-xref"><span>Section 10.9</span></a>.</p>
<p>The other, and indeed frequently used, option is to combine them with explicit multi-step methods into so-called <em>predictor-corrector methods</em>. These work as follows.</p>
<p>Suppose that <span class="math inline">\(\mathbf{w}_0,\ldots,\mathbf{w}_i\)</span> are already known. These are inserted into the difference equation of an explicit method, the <em>(predictor)</em>, which gives an approximation value <span class="math inline">\(\mathbf{w}_{i+1}^{(p)}\)</span>. This value is then inserted into the r.h.s.&nbsp;of the difference equation of an implicit method, the <em>corrector</em>, in order to obtain <span class="math inline">\(\mathbf{w}_{i+1}\)</span>.</p>
<p>To illustrate this, let us consider a frequently used case: the Adams-Bashforth 4-step method as the predictor, combined with the Adams-Moulton 3-step method as the corrector. In each step of the method, we compute the “predicted value” by the explicit method, <span class="math display">\[
\mathbf{w}_{i+1}^{(p)} = \mathbf{w}_i +
      \frac{h}{24} (55 \mathbf{f}_i - 59 \mathbf{f}_{i-1} + 37 \mathbf{f}_{i-2} - 9 \mathbf{f}_{i-3}),
\]</span> and afterwards the “corrected value” using the implicit method: <span class="math display">\[
\mathbf{w}_{i+1} = \mathbf{w}_i + \frac{h}{24} (9 \mathbf{f}(x_{i+1},\mathbf{w}_{i+1}^{(p)}) + 19 \mathbf{f}_i - 5
   \mathbf{f}_{i-1} + \mathbf{f}_{i-2}).
\]</span> Here <span class="math inline">\(\mathbf{f}_j := \mathbf{f}(x_j,\mathbf{w}_j)\)</span>. Both the predictor and the corrector are of order <span class="math inline">\(O(h^4)\)</span>, and so is the entire method, as it turns out. However, the combined predictor-corrector method is more precise than each of the parts alone.</p>
<div class="algorithm">
<div class="algorithmic">
<p><span class="math inline">\(h := (b-a)/N\)</span>, <span class="math inline">\(x_0 := a\)</span>, <span class="math inline">\(\mathbf{w}_0 := \boldsymbol{\alpha}\)</span>, <span class="math inline">\(\mathbf{f}_0 := \mathbf{f}(x_0,\mathbf{w}_0)\)</span>. Compute <span class="math inline">\(\mathbf{w}_i\)</span> using Runge-Kutta fourth order <span class="math inline">\(x_i := x_{i-1}+h\)</span>; <span class="math inline">\(\mathbf{f}_i := \mathbf{f}(x_i,\mathbf{w}_i)\)</span> <span class="math inline">\(x := a + ih\)</span> <span class="math inline">\(\mathbf{w}:= \mathbf{w}_3 +
      \frac{h}{24} (55 \mathbf{f}_3 - 59 \mathbf{f}_2 + 37 \mathbf{f}_1 - 9 \mathbf{f}_0)\)</span> <span class="math inline">\(\mathbf{w}:= \mathbf{w}_3 + \frac{h}{24} (9 \mathbf{f}(x,\mathbf{w}) + 19 \mathbf{f}_3 - 5
   \mathbf{f}_2 + \mathbf{f}_1)\)</span> <span class="math inline">\(x_j := x_{j+1}\)</span>; <span class="math inline">\(\mathbf{w}_j := \mathbf{w}_{j+1}\)</span>; <span class="math inline">\(\mathbf{f}_j :=
\mathbf{f}_{j+1}\)</span>; <span class="math inline">\(x_3 := x\)</span>; <span class="math inline">\(\mathbf{w}_3 := \mathbf{w}\)</span>; <span class="math inline">\(\mathbf{f}_3 := \mathbf{f}(x,\mathbf{w})\)</span>; <span class="math inline">\(\mathbf{w}\)</span></p>
</div>
</div>
<p>Algorithm&nbsp;<a href="#alg:adams4" data-reference-type="ref" data-reference="alg:adams4">[alg:adams4]</a> shows this predictor-corrector method in pseudocode. Lines 3–6 use the classical Runge-Kutta method to set up the required initial values, while lines 7–15 implement the actual multistep method. The scheme is quite efficient, since in each step of the method, only two evaluations of the function <span class="math inline">\(\mathbf{f}\)</span> are used: in line 10 and in line 14 of the algorithm. All other values <span class="math inline">\(\mathbf{f}_j\)</span> are already known from the previous steps.</p>
</section>
<section id="advantages-and-disadvantages-2" class="level3" data-number="10.8.2">
<h3 data-number="10.8.2" class="anchored" data-anchor-id="advantages-and-disadvantages-2"><span class="header-section-number">10.8.2</span> Advantages and disadvantages</h3>
<p>Multistep methods, in particular Adams-Bashforth and Adams-Moulton methods, can be formulated for any error order <span class="math inline">\(O(h^n)\)</span>. However, independent of <span class="math inline">\(n\)</span>, they involve only one evaluation of <span class="math inline">\(\mathbf{f}\)</span> per step (or two for predictor-corrector methods). Compare this with Runge-Kutta methods, where the number of evaluations needed in each step grows approximately like <span class="math inline">\(n\)</span>. Therefore, in particular at high orders, multistep methods tend to be faster than Runge-Kutta methods.</p>
<p>We can naturally combine an explicit and an implicit method of same error order into a predictor-corrector method, which further improves precision. (It should be noted that a similar approach is possible with implicit Runge-Kutta methods, which we have not discussed here.)</p>
<p>However, multistep methods have the disadvantage that they are more difficult to implement. In particular, an additional one-step method is needed in order to compute the extra start values needed for the method.</p>
<p>Like for Taylor and Runge-Kutta methods, the error estimates for Adams-Bashforth and Adams-Moulton methods depends on higher-order derivatives of <span class="math inline">\(\mathbf{f}\)</span>, which are not always small. We will discuss this in <a href="#sec-stiff" class="quarto-xref"><span>Section 10.9</span></a>.</p>
<p>Similar to the Runge-Kutta-Fehlberg algorithm described in <a href="#sec-rkf" class="quarto-xref"><span>Section 10.7</span></a>, we can modify Adams predictor-corrector methods so that they include an automatic choice of the step size. The difference between predicted and corrected value gives a natural indication of the local truncation error. However, the implementation of such a method is not really straightforward, and we do not discuss it in detail here; see for example <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010, sec. 5.7</a>)</span>. One of the difficulties is that, each time when we change the step size <span class="math inline">\(h\)</span>, also the initial values <span class="math inline">\(\boldsymbol{\alpha}_j\)</span> need to be computed again. This can make changing the step size rather costly. It is also possible to generalize multistep methods so that they work with variable <em>order</em>. Again, we do not discuss that here.</p>
</section>
</section>
<section id="sec-stiff" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="sec-stiff"><span class="header-section-number">10.9</span> Stiff equations</h2>
<p>In this section, we will discuss a certain class of initial value problems, called <em>stiff equations</em>, which give rise to particular problems when applying numerical methods to them.</p>
<div class="{exm-stiffivp}">
<p>Let us consider the following IVP as an example: <span id="eq-stiffivp"><span class="math display">\[
   y'(x) = - 20y(x) + 10\cos(2x), \quad 0 \leq x \leq 3, \quad y(0)=1.
\tag{10.88}\]</span></span></p>
<p>This IVP has the exact solution <span class="math display">\[
y(x) =
  \underbrace{\frac{50}{101} \cos(2x) + \frac{5}{101} \sin(2x)}_{\text{steady-state solution}}
  + \underbrace{\frac{51}{101} \exp(-20x)}_{\text{transient solution}} .
\]</span> Note here that one part of the solution, the <em>transient solution</em>, decays very rapidly as <span class="math inline">\(x\)</span> grows. For large <span class="math inline">\(x\)</span>, only the remainder - the <em>steady-state solution</em> - contributes to <span class="math inline">\(y(x)\)</span>.</p>
<p>When applying our approximation methods developed so far to this IVP, one notices the following behaviour. <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Below a certain “critical step size”, our methods give a reasonable approximation of the exact solution, as expected. However, above this step size, the approximation is completely unreliable, and usually grows very rapidly – although the exact solution is bounded by 1. This applies to all methods developed so far, and going to higher-order methods – such as classical Runge-Kutta – does not help.</p>
<p>The reason for this becomes apparent when computing the derivatives of the transient solution: <span class="math display">\[
\frac{ \mathrm{d}^{n}  }{\mathrm{d}x^{n} }  \exp(-20x) = (-20)^n \exp(-20x).
\]</span> This expression can be very large, in particular for high <span class="math inline">\(n\)</span>. Since our error bounds all depended on the supremum of higher-order derivatives of <span class="math inline">\(y(x)\)</span>, this explains the large approximation error.</p>
</div>
<p>ODEs which show this behaviour are called <em>stiff equations</em>. An exact definition of this term is hard to give. Roughly speaking, their crucial property is that they have a transient part of the solution, which decays very rapidly. Looking at the ODE <a href="#eq-stiffivp" class="quarto-xref">Eq.&nbsp;<span>10.88</span></a> directly, this might be seen from the large negative factor in front of <span class="math inline">\(y\)</span>. Of course, in realistic examples, the exact solution of the problem will <em>not</em> be known, the ODE may look more complicated, and one will need to deal with a system of equations rather than with a single equation, so that it is much harder to deduce the “stiff” behaviour from the ODE directly. In applications, it is often apparent from the context whether a problem is stiff or not. Stiff equations take their name from problems in mechanics: a system with “stiff” springs, i.e., with large spring constants and/or strong friction, is typically described by a stiff ODE.</p>
<section id="the-test-equation" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="the-test-equation"><span class="header-section-number">10.9.1</span> The test equation</h3>
<p>In order to understand the problem better, and to decide which numerical methods best to apply, we need a simple but comprehensive test case for our numerical methods. To that end, we apply our methods to the <em>test equation</em> <span id="eq-testeq"><span class="math display">\[
   y'(x) = \lambda y(x), \quad 0 \leq x , \quad y(0)=1,
\tag{10.89}\]</span></span> where <span class="math inline">\(\lambda \in \mathbb{C}\)</span> is a constant, with <span class="math inline">\(\mathop{\mathrm{Re}}\lambda &lt; 0\)</span>. The exact solution is <span class="math display">\[
y(x) = \exp(\lambda x).
\]</span> This solution has <em>only</em> a transient part, which makes it ideal for our purposes.</p>
<p>(We are now dealing with an ODE for a <em>complex</em>-valued function <span class="math inline">\(y\)</span>. This does not really need a generalization of our previous methods: We can always split <span class="math inline">\(y\)</span> into a two-vector <span class="math inline">\((\mathop{\mathrm{Re}}y, \mathop{\mathrm{Im}}y)\)</span>, and thus rewrite our equations as a system of two first-order ODEs with real values. However, the complex-valued notation is convenient here. The nonzero imaginary part of <span class="math inline">\(\lambda\)</span> allows us to include oscillatory solutions as well, with almost no added effort.)</p>
<p>Let us apply the Euler method to the test equation. The difference equation gives <span class="math display">\[
w_{i+1} = w_i + h \,f(x_i,w_i) = w_i + \lambda h w_i = (1 + \lambda h) w_i.
\]</span> With <span class="math inline">\(w_0=1\)</span>, it is then clear that for all <span class="math inline">\(i \in \mathbb{N}\)</span>,</p>
<p><span id="eq-eulerwi"><span class="math display">\[
   w_i = (1+\lambda h)^i.
\tag{10.90}\]</span></span> Thus, if <span class="math inline">\(|1+\lambda h|&gt;1\)</span>, the approximation grows exponentially for large <span class="math inline">\(i\)</span>, and is therefore “grossly wrong” as the exact solution vanishes for large <span class="math inline">\(x\)</span>. If <span class="math inline">\(|1+\lambda h|&lt;1\)</span>, then <span class="math inline">\(w_i\to 0\)</span>, which at least roughly resembles the behaviour of the exact solution.</p>
<p>For our other (explicit) approximation methods, one finds in generalization of <a href="#eq-eulerwi" class="quarto-xref">Eq.&nbsp;<span>10.90</span></a>, <span id="eq-qwi"><span class="math display">\[
   w_i = \bigl(Q(\lambda h)\bigr)^i
\tag{10.91}\]</span></span> with some function <span class="math inline">\(Q\)</span> (for explicit methods, <span class="math inline">\(Q\)</span> is in fact a polynomial). The approximation values grow or decay if <span class="math inline">\(|Q(\lambda h)| &gt; 1\)</span> or <span class="math inline">\(|Q(\lambda h)| &lt; 1\)</span>, respectively.</p>
<p>This motivates us to define the <em>region of stability</em> of an approximation method: <span class="math display">\[
\mathcal{R}:= \big\{\mu \in \mathbb{C} \;\big|\; |Q(\mu)| &lt; 1 \big\}.
\]</span> In order for the approximation to be “reasonable”, i.e.&nbsp;to ensure <span class="math inline">\(w_i \to 0\)</span> as <span class="math inline">\(i \to \infty\)</span>, we then need to choose our step size <span class="math inline">\(h\)</span> so that <span class="math inline">\(h\lambda \in \mathcal{R}\)</span>.</p>
<figure id="fig:stabilitytaylor" class="figure">
<embed src="stability_taylor.pdf">
<figcaption>
Region of stability for Taylor methods
</figcaption>
</figure>
<p>Specifically for Euler’s method, we have <span class="math inline">\(Q(\mu)=1+\mu\)</span>, and <span class="math inline">\(\mathcal{R}\)</span> is a disc with center <span class="math inline">\(-1\)</span> and radius <span class="math inline">\(1\)</span>. For higher-order Taylor methods, the regions of stability can be computed similarly; they are shown in <span class="quarto-unresolved-ref">?fig-stabilitytaylor</span>. For all examples of Runge-Kutta methods discussed in <a href="#sec-rk" class="quarto-xref"><span>Section 10.6</span></a>, the region agrees with that of the Taylor method of the same order. <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>However, from this picture, it appears that <em>all</em> of our methods are vulnerable to the problems posed by stiff equations, as suggested also by the example above. For having a stable method for a large set range of step sizes, we need to look at a different approach. It turns out that certain <em>implicit</em> multi-step methods provide an advantage here.</p>
<p>Let us, in particular, consider the <em>Implicit Trapezoidal method</em>, which is given by the difference equation <span id="eq-trapezde"><span class="math display">\[
\end{aligned}
\tag{10.92}\]</span></span> Applying it to the test equation <a href="#eq-testeq" class="quarto-xref">Eq.&nbsp;<span>10.89</span></a>, we obtain <span class="math display">\[
w_{i+1} = w_i + \frac{h}{2} (\lambda w_{i+1} + \lambda w_i)
  =  \big( 1 + \frac{\lambda h}{2} \big) w_i  + \frac{\lambda h}{2} w_{i+1}.
\]</span> We can solve this for <span class="math inline">\(w_{i+1}\)</span>: <span id="eq-trapezsolve"><span class="math display">\[
w_{i+1} = \frac{2+\lambda h}{2-\lambda h} w_i.
\tag{10.93}\]</span></span> Thus, the result is again of the form <a href="#eq-qwi" class="quarto-xref">Eq.&nbsp;<span>10.91</span></a>, with <span class="math display">\[
Q(\mu) = \frac{2+\mu}{2-\mu}.
\]</span> Since <span class="math display">\[
\begin{gathered}
|Q(\mu)| &lt; 1
    \quad \Leftrightarrow \quad
|2+\mu|^2 &lt; |2-\mu|^2
\\
    \quad \Leftrightarrow \quad
4+ 4 \mathop{\mathrm{Re}}\mu + |\mu|^2
&lt;
4- 4 \mathop{\mathrm{Re}}\mu + |\mu|^2
    \quad \Leftrightarrow \quad
\mathop{\mathrm{Re}}\mu &lt; 0,
\end{gathered}
\]</span> the region of stability <span class="math inline">\(\mathcal{R}\)</span> is just the left half plane. This is the maximum we could hope for: the implicit trapezoidal method gives reasonable results on stiff problems for all step sizes.</p>
<figure id="fig:stabilityadams" class="figure">
<p>
AB4: Adams-Bashforth 4-step; AM3/AM4: Adams-Moulton 3-step/4-step; ABM43: Adams-Bashforth-Moulton predictor-corrector, 4/3-step. Graphics taken from <span class="citation" data-cites="TTM"></span>.
</p>
<img src="stability_multistep_nolegend.jpg" style="width:45.0%" class="figure-img">
<figcaption>
Region of stability for Adams-Bashforth and Adams-Moulton methods
</figcaption>
</figure>
<p>Methods of this kind, i.e., whose region of stability <span class="math inline">\(\mathcal{R}\)</span> contains the entire left half plane, are called <em>absolutely stable</em> or <em>A-stable</em>.</p>
<p>One can apply the above stability analysis to other implicit methods as well, with some added complication – namely, when solving the difference equation as in Eq.&nbsp;<a href="#eq-trapezsolve" class="quarto-xref">Eq.&nbsp;<span>10.93</span></a>, we may in general find several solutions. <span class="quarto-unresolved-ref">?fig-stabilityadams</span> show the resulting regions of stability in some examples. Not all implicit methods are A-stable. In fact, there are no A-stable multistep methods of order <span class="math inline">\(O(h^3)\)</span> or above. Sometimes, one uses a class of implicit multistep methods known as <em>backward differentiation methods (BDM)</em>, which exist for arbitrary error order and come at least close to A-stability; we do not discuss them here.</p>
</section>
<section id="implementing-the-implicit-trapezoidal-method" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="implementing-the-implicit-trapezoidal-method"><span class="header-section-number">10.9.2</span> Implementing the Implicit Trapezoidal Method</h3>
<p>Our analysis so far suggests that we should use the Implicit Trapezoidal method for the numerical treatment of stiff ODEs. However, since the difference equation of the method defines <span class="math inline">\(w_{i+1}\)</span> only implicitly, this poses conceptual problems.</p>
<p>In the extremely simple case of the test equation <a href="#eq-testeq" class="quarto-xref">Eq.&nbsp;<span>10.89</span></a>, we were able to solve the difference equation explicitly for <span class="math inline">\(w_{i+1}\)</span>; see Eq.&nbsp;<a href="#eq-trapezsolve" class="quarto-xref">Eq.&nbsp;<span>10.93</span></a>. We can generalize this to the case of a <em>linear</em> ODE: <span id="eq-ivplinear"><span class="math display">\[
   y'(x) = \ell(x) y(x) + g(x), \quad a \leq x \leq b, \quad y(a)=\alpha.
\tag{10.94}\]</span></span></p>
<p>Here <span class="math inline">\(\ell,g\)</span> are continuous functions from <span class="math inline">\([a,b]\)</span> to <span class="math inline">\(\mathbb{R}\)</span>. In this case, the difference equation <a href="#eq-trapezde" class="quarto-xref">Eq.&nbsp;<span>10.92</span></a> of the implicit trapezoidal method reads, <span class="math display">\[
w_{i+1} = w_i + \frac{h}{2}
  \Big(
    \ell(x_{i+1}) w_{i+1} + g(x_{i+1}) +\ell(x_{i}) w_{i} + g(x_{i})
  \Big).
\]</span> Much like in <a href="#eq-trapezsolve" class="quarto-xref">Eq.&nbsp;<span>10.93</span></a>, this can be solved for <span class="math inline">\(w_{i+1}\)</span>: <span id="eq-trapezsolvelin"><span class="math display">\[
\begin{gathered}
   w_{i+1}
  \Big(
    1 - \frac{h}{2}\ell(x_{i+1})
  \Big)
=
  \Big(
    1 + \frac{h}{2}\ell(x_{i})
  \Big)
   + \frac{h}{2} g(x_{i+1})+ \frac{h}{2} g(x_{i})
%
\\
\Rightarrow
\quad
w_{i+1} = \frac{2 + h \ell(x_{i})}{ 2 - h \ell(x_{i+1})} w_i
+ h \frac{g(x_{i+1}) + g(x_{i}) }{2 - h \ell(x_{i+1})}.
\end{gathered}
\tag{10.95}\]</span></span> Thus, for linear ODEs as in <a href="#eq-ivplinear" class="quarto-xref">Eq.&nbsp;<span>10.94</span></a>, the implicit trapezoidal method can be applied with an explicit difference equation.</p>
<p>For a nonlinear ODE, it will generally not be possible to solve the difference equation symbolically. However, we can try to solve it <em>numerically</em>. As an approach for numerically solving nonlinear equations, we shall use Newton Iteration.</p>
<p>As a reminder: <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Let <span class="math inline">\(F: \mathbb{R} \to \mathbb{R}\)</span> be twice differentiable; suppose we are looking for a solution <span class="math inline">\(x\)</span> of the equation <span class="math display">\[
F(x) = 0.
\]</span> This solution is found with Newton’s method as follows. Starting with <span class="math inline">\(x_0\)</span> sufficiently close to a solution, one recursively defines the sequence <span id="eq-newtondiff"><span class="math display">\[
   x_k := x_{k-1} - \frac{F(x_{k-1})}{F'(x_{k-1})}.
\tag{10.96}\]</span></span> Then <span class="math inline">\(x_k \to x_\infty\)</span> with <span class="math inline">\(F(x_\infty)=0\)</span>.</p>
<p>When implementing this on a computer, one cannot pass to the limit <span class="math inline">\(k \to \infty\)</span>, but needs to stop the iteration when a sufficient precision is reached. In practice, one usually takes the magnitude of the difference term on the r.h.s. of <a href="#eq-newtondiff" class="quarto-xref">Eq.&nbsp;<span>10.96</span></a> as an indicator: The iteration is stopped when <span class="math inline">\(|F(x_{k-1})/F'(x_{k-1})|&lt;T\)</span>, where <span class="math inline">\(T&gt;0\)</span> is the desired tolerance level.</p>
<p>In our situation of the implicit trapezoidal method, we need to solve the equation <span class="math display">\[
w_{i+1} = w_i + \frac{h}{2} \big( f(x_i,w_i)+ f(x_{i+1},w_{i+1})\big).
\]</span> for <span class="math inline">\(w_{i+1}\)</span>, where <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x_{i+1}\)</span>, <span class="math inline">\(w_i\)</span> are given numbers. In other words, we are looking for zeros of the function <span class="math display">\[
F(\hat{w}) = \hat{w} - w_i - \frac{h}{2} \big( f(x_i,w_i)+ f(x_{i+1},\hat{w})\big).
\]</span> Following <a href="#eq-newtondiff" class="quarto-xref">Eq.&nbsp;<span>10.96</span></a>, the following sequence should converge to the solution <span class="math inline">\(w_{i+1}\)</span>: <span class="math display">\[
\begin{aligned}
     \hat{w}_0 &amp;:= w_{i},\\
     \hat{w}_k &amp;:= \hat{w}_{k-1} - \underbrace{\frac{\hat{w}_{k-1} - w_i - \frac{h}{2}\big(
       f(x_i,w_i) + f(x_{i+1},\hat{w}_k)
     \big)}{ 1 - \frac{h}{2} \frac{\partial f}{\partial
     y}(x_{i+1},\hat{w}_{k-1}) }}_{=:v}.
  \end{aligned}
\]</span> We would run this iteration until <span class="math inline">\(|v|&lt;T\)</span>.</p>
<div class="algorithm">
<div class="algorithmic">
<p><span class="math inline">\(x := a\)</span>,  <span class="math inline">\(w := \alpha\)</span>,  <span class="math inline">\(h := (b-a)/N\)</span>; <span class="math inline">\(c := w + \frac{h}{2}f(x,w)\)</span>;  <span class="math inline">\(k:=1\)</span>; <span class="math inline">\(v := \dfrac{ w - c -
h \, f(x+h, w)/2}{ 1 - h\, \partial_y f(x+h, w)/2}\)</span>; <span class="math inline">\(w := w - v\)</span>; <strong>if</strong> <span class="math inline">\(|v|&lt;T\)</span> <strong>then</strong> <strong>break</strong> <strong>end if</strong> <span class="math inline">\(k:=k+1\)</span>; <strong>exception</strong> (“Too many iterations”) <span class="math inline">\(x := x + h\)</span> <span class="math inline">\(w\)</span></p>
</div>
</div>
<p>Algorithm&nbsp;<a href="#alg:trapnewton" data-reference-type="ref" data-reference="alg:trapnewton">[alg:trapnewton]</a> combines the thoughts above, implementing the Implicit Trapezoidal method using Newton’s method. It contains two loops: The outer loop (lines 3–15) is the usual iteration of our one-step and multi-step methods, counting through the mesh points. In each iteration of this loop, there is another loop (lines 5–12) which implements Newton’s method. This inner loop does not have a fixed number of steps a priori. We terminate it either if the desired tolerance is reached (line 8), or (with an error message) if the number of steps exceeds a certain maximum. This is a safety measure to avoid an endless loop in case that the Newton iteration does not converge for any reason.</p>
<p>Both in the case of linear and nonlinear equations, we have so far discussed a single ODE only. For applications in practice, we would need to generalize the methods to systems of ODEs. This is indeed possible, but requires a bit of thought.</p>
<p>In the linear case <a href="#eq-ivplinear" class="quarto-xref">Eq.&nbsp;<span>10.94</span></a>, one would consider a matrix-valued function <span class="math inline">\(\ell\)</span> and a vector-valued <span class="math inline">\(g\)</span>. The difference equation can then still be solved symbolically, like in <a href="#eq-trapezsolvelin" class="quarto-xref">Eq.&nbsp;<span>10.95</span></a>; however, the division by the factor <span class="math inline">\((2-h\ell(x_{i+1}))\)</span> needs to be replaced by a multiplication with an inverse matrix, or equivalently, by solving a system of linear equations. Likewise, in the nonlinear case, we can handle ODE systems but would need the multi-dimensional version of Newton’s method (or, in other words, Newton’s method for systems of nonlinear equations). We shall not discuss the details of this generalization here.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Burden_Faires" class="csl-entry" role="listitem">
Burden, Richard L., and J. Douglas Faires. 2010. <em>Numerical <span>Analysis</span></em>. 9th ed. Brooks Cole.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p><span class="math inline">\(\tau_{i+1}\)</span> obviously depends on <span class="math inline">\(h\)</span>. We do not show this in the notation, however.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Just as a reminder, this is the formula <span class="math inline">\(\sum_{k=0}^{n} r^n = (r^{n+1}-1)/(r-1)\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The same example is considered in <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010, sec. 5.3</a>)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The names of methods may vary in the literature; it is best to compare the tableaux when reading different sources!<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>If you hear someone speaking of “the Runge-Kutta method”, he is probably referring to this <span class="math inline">\(O(h^4)\)</span> method.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This is not necessarily a realistic assumption - but one that simplifies the proof. One can do a similar argument using local bounds on <span class="math inline">\(\mathbf{f}\)</span> and its derivatives in a suitable neighbourhood of the exact solution; however, this introduces formal complications which we want to avoid here.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>We will not deal with any “global” error behaviour here.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Cf. Maple worksheet used in the lecture, available on Moodle<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>This is true for all of our examples, but not for all Runge-Kutta methods in general.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>For more details on Newton’s method for solving (single) nonlinear equations, see Part A of this module, or <span class="citation" data-cites="Burden_Faires">(<a href="references.html#ref-Burden_Faires" role="doc-biblioref">Burden and Faires 2010, sec. 2.3</a>)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter09.html" class="pagination-link  aria-label=" &lt;span="" direct="" method="" for="" solving="" tridiagonal="" linear="" systems&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">A direct method for solving tridiagonal linear systems</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bvp.html" class="pagination-link" aria-label="<span class='chapter-number'>11</span>&nbsp; <span class='chapter-title'>Boundary Value Problems</span>">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Boundary Value Problems</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/gustavdelius/NumericalAnalysis/edit/master/ivp.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>